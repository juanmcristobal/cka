{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introdution","text":""},{"location":"#cluster-architecture-installation-configuration-25","title":"Cluster Architecture, Installation &amp; Configuration (25%)","text":""},{"location":"#workloads-scheduling-15","title":"Workloads &amp; Scheduling (15%)","text":""},{"location":"#services-networking-20","title":"Services &amp; Networking (20%)","text":""},{"location":"#storage-10","title":"Storage (10%)","text":""},{"location":"#troubleshooting-30","title":"Troubleshooting (30%)","text":"<ul> <li>Although no link explicitly focuses on troubleshooting, various topics such as configuring etcd, allocating resources to pods, and network policies may be indirectly related to troubleshooting by properly implementing and configuring these elements.</li> </ul>"},{"location":"#others-not-directly-classified-under-the-above-topics-but-relevant-to-kubernetes-administration","title":"Others (Not directly classified under the above topics but relevant to Kubernetes administration)","text":"<ul> <li>Kubectl Cheat Sheet (Formatting Output)</li> </ul>"},{"location":"18._Kube_Scheduler/","title":"Kube-scheduler","text":"<p>El kube-scheduler juega un rol fundamental en el control plane de Kubernetes al tomar decisiones estrat\u00e9gicas sobre la asignaci\u00f3n de Pods a los Nodos disponibles. Este proceso se basa en la evaluaci\u00f3n de diversas restricciones y la disponibilidad de recursos.</p>"},{"location":"18._Kube_Scheduler/#funcion-del-kube-scheduler","title":"Funci\u00f3n del kube-scheduler","text":"<p>El kube-scheduler es responsable de determinar qu\u00e9 Nodos son opciones v\u00e1lidas para la ubicaci\u00f3n de cada Pod en la cola de programaci\u00f3n, teniendo en cuenta las restricciones y los recursos disponibles. Esto implica:</p> <ul> <li> <p>Evaluar las restricciones: El kube-scheduler verifica las restricciones definidas para los Pods, como requisitos de recursos espec\u00edficos, afinidades y tolerancias, y las compara con las caracter\u00edsticas de los Nodos disponibles.</p> </li> <li> <p>Clasificaci\u00f3n de Nodos: Una vez que se han evaluado las restricciones, el kube-scheduler clasifica cada Nodo v\u00e1lido en funci\u00f3n de una serie de criterios, como la capacidad de recursos, la proximidad a otros servicios o cualquier otra m\u00e9trica relevante.</p> </li> <li> <p>Asignaci\u00f3n de Pods: Finalmente, el kube-scheduler asigna el Pod a un Nodo adecuado, teniendo en cuenta la clasificaci\u00f3n de los Nodos y la distribuci\u00f3n de cargas.</p> </li> </ul>"},{"location":"18._Kube_Scheduler/#diferentes-kube-scheduleres","title":"Diferentes kube-scheduleres","text":"<p>Kubernetes admite m\u00faltiples kube-scheduleres que pueden ser utilizados en un cl\u00faster, y \"kube-scheduler\" es la implementaci\u00f3n de referencia. La capacidad de utilizar diferentes kube-scheduleres proporciona flexibilidad para abordar casos de uso espec\u00edficos o requerimientos de pol\u00edticas de programaci\u00f3n.</p>"},{"location":"18._Kube_Scheduler/#para-saber-mas","title":"Para Saber M\u00e1s","text":"<p>Si deseas obtener informaci\u00f3n m\u00e1s detallada sobre el proceso de programaci\u00f3n y los componentes de \"kube-scheduler\", puedes consultar la documentaci\u00f3n espec\u00edfica sobre programaci\u00f3n en Kubernetes. Esto te proporcionar\u00e1 informaci\u00f3n detallada sobre c\u00f3mo funciona el kube-scheduler y c\u00f3mo personalizar su comportamiento seg\u00fan tus necesidades.</p> <p>En resumen, el kube-scheduler de Kubernetes es una parte esencial del plano de control que se encarga de distribuir los Pods en los Nodos disponibles, asegur\u00e1ndose de que se cumplan las restricciones y los recursos necesarios. La capacidad de utilizar diferentes kube-scheduleres brinda flexibilidad y adaptabilidad en entornos de Kubernetes.</p>"},{"location":"19._Kubelet/","title":"Kubelet","text":"<p>El <code>Kubelet</code> es uno de los componentes clave en un cl\u00faster de Kubernetes y desempe\u00f1a un papel fundamental en la gesti\u00f3n de los nodos en ese cl\u00faster. Kubernetes es una plataforma de orquestaci\u00f3n de contenedores que se utiliza para automatizar la implementaci\u00f3n, el escalado y la administraci\u00f3n de aplicaciones en contenedores en un entorno de cl\u00faster. El <code>Kubelet</code> es el agente que se ejecuta en cada nodo del cl\u00faster y se encarga de administrar los contenedores en ese nodo espec\u00edfico.</p> <p>A continuaci\u00f3n, se describen algunas de las principales funciones y responsabilidades del <code>Kubelet</code>:</p> <ol> <li> <p>Gesti\u00f3n de contenedores: El <code>Kubelet</code> supervisa y gestiona los contenedores que se ejecutan en un nodo espec\u00edfico. Se asegura de que los contenedores est\u00e9n en el estado deseado, ya sea ejecut\u00e1ndose, detenidos o reiniciados seg\u00fan las especificaciones definidas en los objetos de Kubernetes, como los Pods.</p> </li> <li> <p>Comunicaci\u00f3n con el plano de control: El <code>Kubelet</code> act\u00faa como un intermediario entre los componentes del plano de control de Kubernetes y los contenedores en el nodo. Recibe instrucciones y actualizaciones de los componentes maestros de Kubernetes, como el API Server, y se asegura de que los Pods se implementen y funcionen correctamente en el nodo.</p> </li> <li> <p>Monitorizaci\u00f3n de recursos: El <code>Kubelet</code> supervisa el uso de recursos en el nodo, como la CPU, la memoria y el almacenamiento. Si un contenedor consume demasiados recursos o se encuentra en un estado de error, el <code>Kubelet</code> toma medidas para corregir la situaci\u00f3n, como reiniciar el contenedor o notificar al plano de control.</p> </li> <li> <p>Registro de eventos: El <code>Kubelet</code> registra eventos y m\u00e9tricas relacionados con los contenedores y el nodo en el que se ejecuta. Estos registros son \u00fatiles para el diagn\u00f3stico y la soluci\u00f3n de problemas en un cl\u00faster de Kubernetes.</p> </li> <li> <p>Manejo de vol\u00famenes: El <code>Kubelet</code> administra los vol\u00famenes adjuntos a los Pods, garantizando que se monten correctamente en los contenedores. Esto permite el acceso a datos persistentes y compartidos entre los contenedores de un mismo Pod.</p> </li> <li> <p>Soporte para ejecuci\u00f3n de tareas de inicio: El <code>Kubelet</code> puede ejecutar tareas de inicio espec\u00edficas antes de que los contenedores se inicien en un nodo. Esto es \u00fatil para configurar el entorno del nodo antes de que las aplicaciones se ejecuten en \u00e9l.</p> </li> </ol> <p>En resumen, el <code>Kubelet</code> es esencial para garantizar que los Pods y los contenedores se ejecuten correctamente en un nodo dentro de un cl\u00faster de Kubernetes. Trabaja en estrecha colaboraci\u00f3n con otros componentes de Kubernetes para mantener el estado deseado de las aplicaciones y facilitar la administraci\u00f3n de contenedores en un entorno de orquestaci\u00f3n de contenedores a gran escala.</p>"},{"location":"20._Kube_Proxy/","title":"20. Kube Proxy","text":"<p>Kube-Proxy en Kubernetes:</p> <p>Kube-Proxy es un componente crucial en un cl\u00faster de Kubernetes que se encarga de la redireccionar el tr\u00e1fico de red de manera eficiente y gestionar las reglas de red dentro del cl\u00faster. Su objetivo principal es facilitar la comunicaci\u00f3n entre los Pods y los servicios en el cl\u00faster.</p> <ul> <li> <p>Funci\u00f3n principal: Kube-Proxy se encarga de traducir los servicios en reglas de red que permiten el enrutamiento del tr\u00e1fico desde servicios a los Pods correspondientes. Esto es esencial para garantizar que los Pods puedan comunicarse entre s\u00ed y con los servicios de manera fiable.</p> </li> <li> <p>Modos de operaci\u00f3n: Kube-Proxy puede operar en diferentes modos, como IPtables, IPVS (IP Virtual Server), o KernelSpace, y la elecci\u00f3n del modo depende de las necesidades espec\u00edficas del cl\u00faster. Cada modo tiene sus propias caracter\u00edsticas y ventajas.</p> <ol> <li> <p>Modo IPtables: En este modo, Kube-Proxy utiliza iptables, una herramienta ampliamente utilizada en sistemas Linux para configurar reglas de firewall y redirecci\u00f3n de tr\u00e1fico. Las ventajas de este modo incluyen la amplia disponibilidad de iptables en sistemas Linux y su capacidad para gestionar reglas NAT (Network Address Translation) de manera eficiente. Sin embargo, este modo puede volverse menos eficiente a medida que aumenta el n\u00famero de reglas, ya que iptables es secuencial y su rendimiento tiende a disminuir con muchas reglas.</p> </li> <li> <p>Modo IPVS (IP Virtual Server): En este modo, Kube-Proxy utiliza IPVS, que es una tecnolog\u00eda espec\u00edficamente dise\u00f1ada para el balanceo de carga de red. IPVS ofrece un rendimiento m\u00e1s consistente y eficiente en comparaci\u00f3n con iptables, ya que utiliza un algoritmo de b\u00fasqueda de complejidad O(1), lo que significa que el rendimiento no se degrada a medida que aumenta el n\u00famero de reglas. Adem\u00e1s, IPVS admite varios algoritmos de balanceo de carga, como round robin y least connections, lo que proporciona m\u00e1s opciones para el balanceo de carga.</p> </li> <li> <p>Modo KernelSpace: Este modo es espec\u00edfico de nodos Windows y se basa en la plataforma de filtrado virtual (VFP) de Windows. Funciona de manera similar a IPtables en Linux y se encarga de las reglas de red y el enrutamiento en nodos con sistemas Windows. Este modo es relevante para entornos donde se utilizan nodos Windows en lugar de sistemas Linux.</p> </li> </ol> <p>La elecci\u00f3n del modo de operaci\u00f3n de Kube-Proxy depende de tus necesidades espec\u00edficas. Si buscas un rendimiento eficiente y una gesti\u00f3n de reglas de red escalable, IPVS puede ser una elecci\u00f3n s\u00f3lida. Si trabajas en un entorno de nodos Windows, el modo KernelSpace es relevante. Por otro lado, IPtables es una opci\u00f3n s\u00f3lida si deseas utilizar una tecnolog\u00eda ampliamente compatible en sistemas Linux. En \u00faltima instancia, la elecci\u00f3n del modo de operaci\u00f3n debe considerar los requisitos de tu cl\u00faster y las caracter\u00edsticas de tu infraestructura.</p> </li> <li> <p>Instalaci\u00f3n: Kube-Proxy se instala en cada nodo del cl\u00faster. Puede implementarse como un DaemonSet, lo que significa que se ejecuta como un contenedor en cada nodo, garantizando que cada nodo tenga su propia instancia de Kube-Proxy. Tambi\u00e9n es posible ejecutar Kube-Proxy como un proceso independiente directamente en el sistema operativo del nodo.</p> </li> <li> <p>Actualizaciones y reglas de red: Kube-Proxy est\u00e1 en constante comunicaci\u00f3n con el servidor de la API de Kubernetes para mantenerse al tanto de los cambios en los servicios y endpoints. Cuando se producen cambios, Kube-Proxy actualiza las reglas de red en el nodo para asegurarse de que el tr\u00e1fico se redirija correctamente.</p> </li> <li> <p>Balanceo de carga: Kube-Proxy tambi\u00e9n desempe\u00f1a un papel en el balanceo de carga, distribuyendo el tr\u00e1fico entrante a trav\u00e9s de reglas de red adecuadas, lo que garantiza una distribuci\u00f3n equitativa de las solicitudes entre los Pods respaldados por un servicio.</p> </li> <li> <p>Kube-Proxy modes: En el modo IPtables, Kube-Proxy utiliza iptables para realizar la traducci\u00f3n y redirecci\u00f3n de tr\u00e1fico. En el modo IPVS, Kube-Proxy utiliza el servicio IP Virtual Server, que est\u00e1 optimizado para el balanceo de carga. En el modo KernelSpace, espec\u00edfico de nodos Windows, utiliza la plataforma de filtrado virtual (VFP) de Windows.</p> </li> <li> <p>Verificaci\u00f3n del modo de operaci\u00f3n: Los administradores de cl\u00fasteres pueden verificar el modo de operaci\u00f3n de Kube-Proxy accediendo a la informaci\u00f3n del propio Kube-Proxy, lo que les permite comprender c\u00f3mo se est\u00e1n aplicando las reglas de red.</p> </li> </ul> <p>En resumen, Kube-Proxy es un componente cr\u00edtico de Kubernetes que permite una comunicaci\u00f3n eficiente y fiable entre los Pods y los servicios en un cl\u00faster, y se adapta a diferentes modos de operaci\u00f3n seg\u00fan las necesidades de la configuraci\u00f3n del cl\u00faster.</p>"},{"location":"24.Pods/","title":"Pods en Kubernetes","text":"<p>Los Pods son la unidad m\u00e1s peque\u00f1a y b\u00e1sica en Kubernetes. Representan una instancia \u00fanica de un proceso en un cl\u00faster de Kubernetes. Los Pods pueden contener uno o m\u00e1s contenedores que comparten recursos y espacio de red en el mismo entorno.</p>"},{"location":"24.Pods/#caracteristicas-clave-de-los-pods","title":"Caracter\u00edsticas clave de los Pods:","text":"<ol> <li> <p>Unidad de Despliegue: Los Pods son la unidad de despliegue m\u00e1s peque\u00f1a en Kubernetes. Se utilizan para implementar una o varias instancias de una aplicaci\u00f3n o servicio.</p> </li> <li> <p>Contenedores Acoplados: Un Pod puede contener uno o varios contenedores que comparten el mismo espacio de red y recursos. Esto permite que los contenedores dentro del mismo Pod se comuniquen entre s\u00ed a trav\u00e9s de localhost.</p> </li> <li> <p>Recursos Compartidos: Los contenedores en un Pod comparten recursos como el almacenamiento y las direcciones IP. Esto es \u00fatil cuando se necesita compartir datos o configuraciones entre los contenedores.</p> </li> <li> <p>Escalabilidad: Los Pods se pueden escalar horizontalmente creando m\u00faltiples r\u00e9plicas de un mismo Pod para distribuir la carga de trabajo.</p> </li> <li> <p>Vol\u00famenes: Los Pods pueden incluir vol\u00famenes que permiten el almacenamiento de datos persistente o compartido entre los contenedores.</p> </li> </ol>"},{"location":"24.Pods/#creacion-de-pods","title":"Creaci\u00f3n de Pods","text":"<p>Para crear un Pod en Kubernetes, hay dos m\u00e9todos comunes:</p>"},{"location":"24.Pods/#1-creacion-con-un-archivo-yaml","title":"1. Creaci\u00f3n con un archivo YAML","text":"<p>Para crear un Pod, se utiliza un archivo de configuraci\u00f3n YAML que describe las propiedades del Pod. El archivo YAML debe incluir:</p> <ul> <li><code>apiVersion</code>: La versi\u00f3n de la API de Kubernetes que se est\u00e1 utilizando.</li> <li><code>kind</code>: El tipo de objeto que se est\u00e1 creando, que en este caso es \"Pod\".</li> <li><code>metadata</code>: Informaci\u00f3n sobre el Pod, como su nombre y etiquetas.</li> <li><code>spec</code>: Las especificaciones del Pod, incluyendo los contenedores que se ejecutar\u00e1n en \u00e9l.</li> </ul>"},{"location":"24.Pods/#ejemplo-de-archivo-yaml-para-un-pod","title":"Ejemplo de archivo YAML para un Pod:","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mi-pod\nspec:\n  containers:\n  - name: contenedor-1\n    image: imagen-1:latest\n  - name: contenedor-2\n    image: imagen-2:latest\n</code></pre> <p>En este ejemplo, se crea un Pod llamado \"mi-pod\" que contiene dos contenedores, \"contenedor-1\" y \"contenedor-2\".</p>"},{"location":"24.Pods/#2-creacion-manual-por-linea-de-comandos","title":"2. Creaci\u00f3n manual por l\u00ednea de comandos","text":"<p>Tambi\u00e9n puedes crear un Pod directamente desde la l\u00ednea de comandos utilizando <code>kubectl</code>. Aqu\u00ed hay un ejemplo:</p> <pre><code>kubectl run mi-pod --image=imagen-1:latest\n</code></pre> <p>En este ejemplo, se crea un Pod llamado \"mi-pod\" con un \u00fanico contenedor utilizando la imagen \"imagen-1:latest\". Esta es una forma r\u00e1pida de crear un Pod para pruebas o tareas temporales.</p>"},{"location":"24.Pods/#gestion-de-pods","title":"Gesti\u00f3n de Pods","text":"<p>Los Pods se pueden administrar utilizando la herramienta de l\u00ednea de comandos <code>kubectl</code>. Algunas operaciones comunes incluyen:</p> <ul> <li>Crear un Pod: <code>kubectl create -f archivo.yaml</code> o <code>kubectl run nombre-del-pod --image=imagen:tag</code></li> <li>Listar Pods: <code>kubectl get pods</code></li> <li>Obtener informaci\u00f3n detallada sobre un Pod: <code>kubectl describe pod nombre-del-pod</code></li> <li>Escalar un Pod: <code>kubectl scale --replicas=n deployment/nombre-del-despliegue</code></li> </ul>"},{"location":"24.Pods/#monitoreo-y-mantenimiento","title":"Monitoreo y Mantenimiento","text":"<p>Una vez que los Pods est\u00e1n en funcionamiento, es importante monitorear su estado y rendimiento. Puedes utilizar herramientas de monitoreo y registros, como Prometheus y Grafana, para supervisar los Pods.</p> <p>Tambi\u00e9n puedes aplicar actualizaciones y cambios en los Pods a trav\u00e9s de estrategias de despliegue, como Rolling Updates, para garantizar la disponibilidad continua de la aplicaci\u00f3n.</p> <p>Los Pods en Kubernetes son la base para implementar aplicaciones y servicios, y entender c\u00f3mo trabajar con ellos es esencial para operar un cl\u00faster de Kubernetes de manera efectiva.</p>"},{"location":"26.ReplicaSet/","title":"ReplicaSets en Kubernetes","text":"<p>En Kubernetes, un ReplicaSet es un controlador que garantiza que un n\u00famero especificado de r\u00e9plicas de Pods est\u00e9n en funcionamiento en todo momento. Est\u00e1 dise\u00f1ado para mantener la alta disponibilidad de las aplicaciones y garantizar que el n\u00famero deseado de Pods est\u00e9 en ejecuci\u00f3n, incluso si se produce un fallo.</p>"},{"location":"26.ReplicaSet/#caracteristicas-clave","title":"Caracter\u00edsticas Clave","text":"<ul> <li>Un ReplicaSet define el n\u00famero deseado de r\u00e9plicas de un Pod espec\u00edfico.</li> <li>Si el n\u00famero actual de r\u00e9plicas es inferior al deseado, el ReplicaSet crea autom\u00e1ticamente nuevas r\u00e9plicas.</li> <li>Si hay m\u00e1s r\u00e9plicas que las deseadas, el ReplicaSet elimina el exceso.</li> <li>Los ReplicaSets utilizan etiquetas (labels) para seleccionar los Pods que gestionan y garantizar que coincidan con los criterios especificados.</li> </ul>"},{"location":"26.ReplicaSet/#creando-un-replicaset","title":"Creando un ReplicaSet","text":"<p>Aqu\u00ed hay un ejemplo de c\u00f3mo crear un ReplicaSet en un archivo YAML:</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: mi-replicaset\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mi-aplicacion\n  template:\n    metadata:\n      labels:\n        app: mi-aplicacion\n    spec:\n      containers:\n      - name: mi-contenedor\n        image: mi-imagen:1.0\n</code></pre> <ul> <li><code>apiVersion</code>: Especifica la versi\u00f3n de la API de Kubernetes que se est\u00e1 utilizando (en este caso, \"apps/v1\").</li> <li><code>kind</code>: Define el tipo del objeto, que es \"ReplicaSet\".</li> <li><code>metadata</code>: Contiene metadatos como el nombre del ReplicaSet.</li> <li><code>spec</code>: Aqu\u00ed se especifica el n\u00famero deseado de r\u00e9plicas, el selector para elegir los Pods y la plantilla para la creaci\u00f3n de Pods.</li> </ul>"},{"location":"26.ReplicaSet/#uso-de-replicasetsx","title":"Uso de ReplicaSetsx","text":"<p>Los ReplicaSets son ampliamente utilizados en Kubernetes para garantizar que las aplicaciones est\u00e9n disponibles y funcionando de manera confiable. Algunos puntos clave sobre su uso incluyen:</p> <ul> <li>Los ReplicaSets son a menudo gestionados por controladores de nivel superior como Deployments.</li> <li>Cuando se actualiza una aplicaci\u00f3n, se crea un nuevo ReplicaSet con la versi\u00f3n actualizada y se escalan las r\u00e9plicas de acuerdo a las necesidades.</li> <li>Los ReplicaSets reemplazan a los Replication Controllers en escenarios modernos de Kubernetes.</li> </ul>"},{"location":"26.ReplicaSet/#comandos-utiles","title":"Comandos \u00datiles","text":"<ul> <li>Crear un ReplicaSet desde un archivo YAML: <code>kubectl apply -f mi-replicaset.yaml</code></li> <li>Ver todos los ReplicaSets en el cl\u00faster: <code>kubectl get replicasets</code></li> <li>Ver detalles de un ReplicaSet espec\u00edfico: <code>kubectl describe replicaset nombre-del-replicaset</code></li> <li>Escalar un ReplicaSet: <code>kubectl scale --replicas=N replicaset nombre-del-replicaset</code></li> </ul> <p>Los ReplicaSets son una parte fundamental de la orquestaci\u00f3n de aplicaciones en Kubernetes y desempe\u00f1an un papel crucial en el mantenimiento de la disponibilidad y la escalabilidad de las aplicaciones.</p>"},{"location":"27.Deployments/","title":"Deployments en Kubernetes","text":"<p>Los Deployments en Kubernetes son recursos que se utilizan para administrar la implementaci\u00f3n de aplicaciones y actualizaciones de forma declarativa. Proporcionan una abstracci\u00f3n m\u00e1s alta que los ReplicaSets, facilitando la administraci\u00f3n y el despliegue de aplicaciones en cl\u00fasteres de Kubernetes. Los Deployments aseguran que una aplicaci\u00f3n se ejecute de manera confiable, garantizando el n\u00famero deseado de r\u00e9plicas y manejando actualizaciones y cambios sin tiempo de inactividad.</p>"},{"location":"27.Deployments/#que-es-deployment","title":"\u00bfQu\u00e9 es Deployment?","text":"<p>La herramienta Deployment se define como un controlador de la plataforma que tiene la labor de ofrecer actualizaciones declarativas enfocadas en los ReplicaSets y pods disponibles. Cuando se establece un estado deseado en un objeto de esta opci\u00f3n, Deployment se encarga de llevar a cabo, de una manera controlada, la transici\u00f3n entre el estado actual en el que se encuentra el objeto hacia el estado deseado indicado por el usuario. Esto implica que los pods que est\u00e9n a cargo de este controlador deben alcanzar dicho estado.</p>"},{"location":"27.Deployments/#caracteristicas-de-deployment","title":"Caracter\u00edsticas de Deployment","text":"<p>Dentro de los principales elementos caracter\u00edsticos de Deployment, se encuentra que este tambi\u00e9n puede definirse para otras labores, como la creaci\u00f3n de nuevos recursos de ReplicaSets, o bien eliminar la totalidad de los Deployments que existan en el sistema, al tiempo que adopta todos sus recursos con nuevos controladores de Deployment.</p> <p>Se recomienda, adem\u00e1s, no gestionar de manera directa los recursos de ReplicaSets que forman parte de un Deployment.</p> <p>Otra de las caracter\u00edsticas de este controlador es que tiene la capacidad de indicarle al sistema de Kubernetes c\u00f3mo se debe realizar la creaci\u00f3n o edici\u00f3n de las instancias de los recursos de pods que incluyan una aplicaci\u00f3n contenerizada.</p> <p>Este controlador tambi\u00e9n se caracteriza por contar con la posibilidad de escalar la cantidad de los pods de r\u00e9plica, dependiendo de las necesidades de las infraestructuras del usuario, as\u00ed como contribuir a que se implemente la actualizaci\u00f3n del c\u00f3digo controladamente.</p>"},{"location":"27.Deployments/#uso-de-deployments","title":"Uso de Deployments","text":"<p>Los Deployments son esenciales en la gesti\u00f3n de aplicaciones en Kubernetes. Algunos puntos clave sobre su uso incluyen:</p> <ul> <li>Los Deployments gestionan ReplicaSets bajo el cap\u00f3 para mantener el n\u00famero deseado de r\u00e9plicas.</li> <li>Cuando se actualiza una aplicaci\u00f3n, se crea un nuevo ReplicaSet y se eliminan gradualmente las r\u00e9plicas antiguas para minimizar el tiempo de inactividad.</li> </ul>"},{"location":"27.Deployments/#comandos-y-ejemplos","title":"Comandos y Ejemplos","text":"<p>A continuaci\u00f3n, se muestran algunos comandos y ejemplos de uso de Deployments en Kubernetes:</p> <ol> <li>Crear un Deployment:</li> </ol> <p><code>bash    kubectl create deployment my-app --image=my-image:latest</code></p> <p>Esto crea un Deployment llamado <code>my-app</code> utilizando la imagen <code>my-image:latest</code>.</p> <ol> <li>Escalar un Deployment:</li> </ol> <p><code>bash    kubectl scale deployment my-app --replicas=3</code></p> <p>Esto escala el Deployment <code>my-app</code> para que tenga 3 r\u00e9plicas en funcionamiento.</p> <ol> <li>Actualizar un Deployment:</li> </ol> <p><code>bash    kubectl set image deployment/my-app my-app=my-new-image:latest</code></p> <p>Esto actualiza la imagen utilizada por el Deployment <code>my-app</code> a <code>my-new-image:latest</code>.</p> <ol> <li>Describir un Deployment:</li> </ol> <p><code>bash    kubectl describe deployment my-app</code></p> <p>Muestra detalles y eventos del Deployment <code>my-app</code>.</p>"},{"location":"27.Deployments/#casos-de-uso-de-deployment","title":"Casos de uso de Deployment","text":"<p>Deployment se caracteriza, adem\u00e1s, por permitir una serie de casos de uso diferentes dentro de la plataforma. Todos estos casos deber\u00edan cubrirse al usar el objeto de Deployment. Algunos de los casos de uso que normalmente lleva a cabo el controlador de Deployment son:</p> <ul> <li> <p>Despliegue de ReplicaSet: Uno de los casos de uso de este controlador se da cuando el ReplicaSet se encarga de la creaci\u00f3n de los recursos de pods que funcionan en un segundo plano. Adem\u00e1s de esto, el controlador de Deployment cumple la funci\u00f3n de verificar el estado de despliegue del recurso con el objetivo de comprobar que este sea el ideal o no.</p> </li> <li> <p>Limpieza de ReplicaSet: Otro de los casos de uso del controlador de Deployment es que permite limpiar los recursos de ReplicaSet que sean m\u00e1s antiguos y que ya no se necesiten en el sistema.</p> </li> <li> <p>Escalado: Deployment tambi\u00e9n se caracteriza por permitir el escalado de tipo horizontal, lo que contribuye a que el controlador aumente su capacidad para soportar las diferentes cargas de trabajo.</p> </li> </ul>"},{"location":"36.Services/","title":"Servicios en Kubernetes","text":"<p>Los servicios son una parte fundamental de la infraestructura de Kubernetes. Proporcionan una abstracci\u00f3n para la conectividad de red a las aplicaciones que se ejecutan en cl\u00fasteres de Kubernetes. Los servicios permiten que las aplicaciones se comuniquen entre s\u00ed y con el mundo exterior de manera confiable y escalable.</p>"},{"location":"36.Services/#concepto-basico","title":"Concepto B\u00e1sico","text":"<p>Un servicio en Kubernetes es una abstracci\u00f3n que define un conjunto l\u00f3gico de pods y una pol\u00edtica por la cual acceder a ellos. Proporciona una direcci\u00f3n IP y un puerto de servicio \u00fanicos que representan un conjunto de pods. Los servicios permiten que las aplicaciones se conecten a otros componentes de la aplicaci\u00f3n sin conocer la ubicaci\u00f3n exacta de los pods subyacentes.</p> <p>Los servicios se utilizan principalmente para:</p> <ol> <li>Exponer aplicaciones internas o externas a trav\u00e9s de una red controlada.</li> <li>Descubrir otros servicios dentro del cl\u00faster.</li> <li>Distribuir el tr\u00e1fico de red a trav\u00e9s de varios pods que respaldan una aplicaci\u00f3n.</li> </ol>"},{"location":"36.Services/#tipos-de-servicios","title":"Tipos de Servicios","text":"<p>Kubernetes ofrece varios tipos de servicios para satisfacer diferentes necesidades:</p> <ol> <li> <p>ClusterIP: Este es el tipo de servicio predeterminado. Expondr\u00e1 el servicio solo dentro del cl\u00faster y proporcionar\u00e1 una direcci\u00f3n IP interna. Es \u00fatil para la comunicaci\u00f3n interna entre los pods.</p> </li> <li> <p>NodePort: Expondr\u00e1 el servicio en un puerto espec\u00edfico en cada nodo del cl\u00faster. Esto permite que el servicio sea accesible desde fuera del cl\u00faster a trav\u00e9s de la direcci\u00f3n IP de cualquier nodo y el puerto especificado.</p> </li> <li> <p>LoadBalancer: Este tipo de servicio se utiliza en entornos en la nube y configura autom\u00e1ticamente un balanceador de carga para el servicio. El balanceador distribuye el tr\u00e1fico entre los pods del servicio.</p> </li> <li> <p>ExternalName: Crea un alias para un servicio fuera del cl\u00faster. Esto se utiliza cuando se necesita acceder a un servicio externo a trav\u00e9s de un nombre DNS.</p> </li> </ol>"},{"location":"36.Services/#selectores-y-etiquetas","title":"Selectores y Etiquetas","text":"<p>Los servicios se asocian a pods mediante etiquetas (labels) y selectores. Un servicio selecciona pods que coinciden con etiquetas espec\u00edficas y los enruta autom\u00e1ticamente.</p> <p>Ejemplo de etiquetas y selectores:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre> <p>En este ejemplo, el servicio \"my-service\" selecciona todos los pods con la etiqueta \"app: my-app\" y los enruta al puerto 9376.</p>"},{"location":"36.Services/#uso-de-dns","title":"Uso de DNS","text":"<p>Kubernetes proporciona un sistema de DNS interno para facilitar la resoluci\u00f3n de nombres de servicio. Los servicios se registran autom\u00e1ticamente en el DNS y se pueden acceder utilizando sus nombres. Por ejemplo, si tienes un servicio llamado \"my-service,\" puedes acceder a \u00e9l como \"my-service\" desde cualquier pod en el cl\u00faster.</p>"},{"location":"36.Services/#escalabilidad","title":"Escalabilidad","text":"<p>Los servicios en Kubernetes son escalables. Si tienes m\u00faltiples pods que respaldan una aplicaci\u00f3n, el servicio distribuir\u00e1 autom\u00e1ticamente el tr\u00e1fico a trav\u00e9s de todos los pods disponibles. A medida que se escalan los pods, el servicio se adapta y enruta el tr\u00e1fico a los nuevos pods.</p>"},{"location":"36.Services/#creacion-de-servicios","title":"Creaci\u00f3n de Servicios","text":"<p>Puedes crear servicios en Kubernetes utilizando archivos de definici\u00f3n YAML o mediante comandos <code>kubectl</code>. Aqu\u00ed hay un ejemplo de un archivo YAML que define un servicio de tipo ClusterIP:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre> <p>Para crear este servicio, puedes usar el comando:</p> <pre><code>kubectl apply -f service-definition.yaml\n</code></pre>"},{"location":"36.Services/#eliminacion-de-servicios","title":"Eliminaci\u00f3n de Servicios","text":"<p>Para eliminar un servicio, puedes utilizar el comando <code>kubectl delete</code> o eliminar el archivo de definici\u00f3n YAML:</p> <pre><code>kubectl delete service my-service\n</code></pre>"},{"location":"41.Namespaces/","title":"41.Namespaces","text":""},{"location":"41.Namespaces/#que-son-los-namespaces-en-kubernetes","title":"\u00bfQu\u00e9 son los Namespaces en Kubernetes?","text":"<p>Los Namespaces son una forma de dividir un cl\u00faster de Kubernetes en m\u00faltiples cl\u00fasteres virtuales l\u00f3gicos. Cada espacio de nombres es como un cl\u00faster de Kubernetes independiente que contiene sus propios recursos y objetos. Esto permite la segmentaci\u00f3n y el aislamiento de aplicaciones, servicios y recursos dentro de un mismo cl\u00faster. Los Namespaces se utilizan para evitar conflictos y organizar recursos de manera m\u00e1s eficiente.</p>"},{"location":"41.Namespaces/#usos-comunes-de-los-namespaces","title":"Usos Comunes de los Namespaces","text":"<ol> <li> <p>Aislamiento de Recursos: Los Namespaces permiten a equipos o usuarios separar sus recursos de otros equipos o usuarios en el mismo cl\u00faster. Cada espacio de nombres tiene su propio conjunto de objetos, como pods, servicios y vol\u00famenes persistentes.</p> </li> <li> <p>Desarrollo y Producci\u00f3n: Los Namespaces son \u00fatiles para separar el entorno de desarrollo del de producci\u00f3n. Puedes tener un espacio de nombres para el desarrollo y otro para la producci\u00f3n, lo que facilita la gesti\u00f3n de ciclos de vida de aplicaciones.</p> </li> <li> <p>Multitenancy: En entornos de m\u00faltiples inquilinos, los Namespaces se utilizan para aislar aplicaciones de diferentes inquilinos en un cl\u00faster compartido.</p> </li> <li> <p>Seguridad y Pol\u00edticas: Los Namespaces permiten la aplicaci\u00f3n de pol\u00edticas de seguridad y acceso espec\u00edficas a cada segmento de recursos.</p> </li> </ol>"},{"location":"41.Namespaces/#namespaces-predeterminados","title":"Namespaces Predeterminados","text":"<p>Kubernetes proporciona algunos Namespaces predeterminados, incluyendo:</p> <ul> <li><code>default</code>: El espacio de nombres predeterminado donde se crean los recursos si no se especifica un espacio de nombres.</li> <li><code>kube-system</code>: Utilizado por los componentes internos del sistema de Kubernetes, como el programador y el controlador de replicaci\u00f3n.</li> <li><code>kube-public</code>: Accesible p\u00fablicamente y utilizado para recursos que deben estar disponibles para todos.</li> </ul>"},{"location":"41.Namespaces/#creacion-y-gestion-de-namespaces","title":"Creaci\u00f3n y Gesti\u00f3n de Namespaces","text":"<p>Puedes crear Namespaces utilizando archivos de definici\u00f3n YAML o mediante comandos ``kubectl``. Aqu\u00ed tienes un ejemplo de un archivo YAML para crear un espacio de nombres:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: mi-namespace\n</code></pre> <p>Para crear este espacio de nombres, puedes usar el comando:</p> <pre><code>kubectl create -f namespace-definition.yaml\n</code></pre>"},{"location":"41.Namespaces/#cambio-de-espacio-de-nombres","title":"Cambio de Espacio de Nombres","text":"<p>Puedes cambiar de espacio de nombres en <code>kubectl</code> utilizando la opci\u00f3n <code>--namespace</code> o <code>-n</code>. Por ejemplo:</p> <pre><code>kubectl get pods --namespace=mi-namespace\n</code></pre>"},{"location":"41.Namespaces/#verificacion-de-recursos-en-un-espacio-de-nombres","title":"Verificaci\u00f3n de Recursos en un Espacio de Nombres","text":"<p>Puedes verificar los recursos en un espacio de nombres espec\u00edfico utilizando comandos como <code>kubectl get</code>, <code>kubectl describe</code> y <code>kubectl delete</code> con la opci\u00f3n <code>--namespace</code>.</p>"},{"location":"41.Namespaces/#eliminacion-de-namespaces","title":"Eliminaci\u00f3n de Namespaces","text":"<p>Puedes eliminar un espacio de nombres y todos los recursos asociados con \u00e9l utilizando el comando:</p> <pre><code>kubectl delete namespace mi-namespace\n</code></pre>"},{"location":"41.Namespaces/#roles-y-permisos","title":"Roles y Permisos","text":"<p>Kubernetes utiliza roles y reglas de autorizaci\u00f3n basadas en recursos y Namespaces para controlar el acceso a los recursos. Esto permite definir qui\u00e9n puede realizar acciones en recursos espec\u00edficos en un espacio de nombres dado.</p>"},{"location":"41.Namespaces/#resumen","title":"Resumen","text":"<p>Los Namespaces en Kubernetes son una caracter\u00edstica esencial para la organizaci\u00f3n y el aislamiento de recursos en un cl\u00faster. Facilitan la gesti\u00f3n de entornos multitenant, la organizaci\u00f3n de recursos y la aplicaci\u00f3n de pol\u00edticas de seguridad. Comprender c\u00f3mo crear, cambiar y eliminar Namespaces, as\u00ed como c\u00f3mo gestionar roles y permisos, es fundamental para administrar cl\u00fasteres de Kubernetes de manera efectiva. ```</p>"},{"location":"quick-reference-to-doc/","title":"T\u00edtulo de la P\u00e1gina","text":""},{"location":"quick-reference-to-doc/#cluster-architecture-installation-configuration","title":"Cluster Architecture, Installation &amp; Configuration","text":"Topic keywords etcd etcd \ud83d\udd0d\ufe0f Create Secrets \ud83d\udcbb\ufe0f <code>kubectl create secret generic -h</code> Create a namespace with the specified name. \ud83d\udcbb\ufe0f <code>kubectl create namespace -h</code> create configmap from ... \ud83d\udcbb\ufe0f <code>kubectl create configmap -h</code> Co    ntenido 3.1 Contenido 3.2"},{"location":"quick-reference-to-doc/#workloads-scheduling","title":"Workloads &amp; Scheduling","text":"Topic Tip Create more complex pod \ud83d\udcdd Create a simple Pod manifest and edit   If you need extra help  Pods limits CPU/MEM limits \ud83d\udd0d\ufe0f Rollback something undo \ud83d\udd0d\ufe0f \ud83d\udcbb\ufe0f <code>kubectl rollout -h</code> examples Create DaemonSet \ud83d\udcdd Edit file, change Deployment to DaemonSet, and remove surplus fields.  to have the daemonset runnable on control plane search Daemonset \ud83d\udd0d\ufe0f Contenido 3.1 Contenido 3.2 Contenido 3.1 Contenido 3.2"},{"location":"quick-reference-to-doc/#others","title":"Others","text":"Topic Tip List Resources \ud83d\udcbb\ufe0f <code>kubectl get -h</code> help!  custom-colum \ud83d\udd0d\ufe0f jsonpath \ud83d\udd0d\ufe0f Filter by label Contenido 3.1 Contenido 3.2 <p>extractions custom-columns</p>"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/","title":"Actualizaci\u00f3n de Kubernetes con kubeadm","text":"<p>Este documento proporciona una gu\u00eda paso a paso para actualizar un cl\u00faster de Kubernetes utilizando <code>kubeadm</code>.</p>"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/#verificar-la-version-actual-de-kubernetes","title":"Verificar la Versi\u00f3n Actual de Kubernetes","text":"<p>Para verificar la versi\u00f3n actual de Kubernetes en los nodos del cl\u00faster, utiliza el siguiente comando:</p> <p>```shell kubectl get nodes ```</p> <p>Ejemplo de salida:</p> <p>```plaintext NAME                          STATUS   ROLES           AGE    VERSION aux01-kubadm-vagrant          Ready              2d6h   v1.26.1 aux02-kubadm-vagrant          Ready              2d6h   v1.26.1 controlplane-kubadm-vagrant   Ready    control-plane   2d6h   v1.26.1 ```"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/#verificar-versiones-disponibles-para-instalar","title":"Verificar Versiones Disponibles para Instalar","text":"<p>Para listar todas las versiones disponibles de `kubeadm`, ejecuta:</p> <p>```shell apt list -a kubeadm ```</p>"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/#preparacion-para-la-actualizacion","title":"Preparaci\u00f3n para la Actualizaci\u00f3n","text":"<ol> <li>Desbloquear `kubeadm` para la Instalaci\u00f3n:</li> </ol> <p>```shell    sudo apt-mark unhold kubeadm    ```</p> <ol> <li>Actualizar a la Versi\u00f3n Deseada (ejemplo: 1.26.9):</li> </ol> <p>```shell    sudo apt-get update &amp;&amp; apt-get install -y kubeadm=1.26.9-00    ```</p> <ol> <li>Bloquear `kubeadm` Despu\u00e9s de la Instalaci\u00f3n:</li> </ol> <p>```shell    sudo apt-mark hold kubeadm    ```</p> <ol> <li>Verificar la Versi\u00f3n Actualizada de `kubeadm`:</li> </ol> <p>```shell    kubeadm version    ```</p>"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/#plan-de-actualizacion","title":"Plan de Actualizaci\u00f3n","text":"<p>Para verificar el plan de actualizaci\u00f3n y las versiones objetivo, utiliza:</p> <p>```shell sudo kubeadm upgrade plan ```</p> <p>Este comando te proporcionar\u00e1 detalles sobre las versiones actuales y las versiones objetivo de los componentes del cl\u00faster.</p>"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/#aplicar-la-actualizacion","title":"Aplicar la Actualizaci\u00f3n","text":"<ol> <li>Aplicar la Actualizaci\u00f3n con `kubeadm`:</li> </ol> <p>```shell    sudo kubeadm upgrade apply v1.26.9    ```</p> <ol> <li>Preparar el Nodo para Mantenimiento:</li> </ol> <p>```shell    kubectl drain controlplane-kubadm-vagrant --ignore-daemonsets    ```</p> <ol> <li>Actualizar `kubectl` y `kubelet`:</li> </ol> <p>```shell    apt-mark unhold kubelet kubectl &amp;&amp; \\    apt-get update &amp;&amp; apt-get install -y kubelet='1.26.9-00' kubectl='1.26.9-00' &amp;&amp; \\    apt-mark hold kubelet kubectl    ```</p> <ol> <li>Reiniciar `kubelet`:</li> </ol> <p>```shell    sudo systemctl daemon-reload    sudo systemctl restart kubelet    ```</p>"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/#verificar-la-actualizacion","title":"Verificar la Actualizaci\u00f3n","text":"<p>Verifica que los nodos est\u00e9n actualizados y listos:</p> <p>```shell kubectl get nodes ```</p>"},{"location":"cluster-architecture-installation-and-configuration/01.Actualizaci%C3%B3n%20-%20Nodo%20Principal%20/#reanudar-programaciones-en-el-nodo","title":"Reanudar Programaciones en el Nodo","text":"<p>Despu\u00e9s de la actualizaci\u00f3n, puedes volver a marcar el nodo como programable:</p> <p>```shell kubectl uncordon controlplane-kubadm-vagrant ```</p> <ul> <li>ver la versi\u00f3n de kubernetes</li> </ul> <pre><code>vagrant@controlplane-kubadm-vagrant:~$ kubectl get nodes\nNAME                          STATUS   ROLES           AGE    VERSION\naux01-kubadm-vagrant          Ready    &lt;none&gt;          2d6h   v1.26.1\naux02-kubadm-vagrant          Ready    &lt;none&gt;          2d6h   v1.26.1\ncontrolplane-kubadm-vagrant   Ready    control-plane   2d6h   v1.26.1\n</code></pre> <ul> <li>ver las versiones disponibles a instalar</li> </ul> <pre><code>apt list -a kubeadm\n</code></pre> <ul> <li>desbloqueamos kubeadm para instalar</li> </ul> <p>vagrant@controlplane-kubadm-vagrant:~$ sudo apt-mark unhold kubeadm Canceled hold on kubeadm.</p> <p>-Actualizo a la 1.26.9</p> <p>sudo apt-get update &amp;&amp; apt-get install -y kubeadm=1.26.9-00</p> <ul> <li> <p>bloqueo kubeadm vagrant@controlplane-kubadm-vagrant:~$ sudo apt-mark hold kubeadm kubeadm set on hold.</p> </li> <li> <p>reviso version</p> </li> </ul> <p>vagrant@controlplane-kubadm-vagrant:~$ kubeadm version kubeadm version: &amp;version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9\", GitCommit:\"d1483fdf7a0578c83523bc1e2212a606a44fd71d\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T11:31:28Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}</p> <p>vagrant@controlplane-kubadm-vagrant:~$ sudo kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.26.1 [upgrade/versions] kubeadm version: v1.26.9 I1129 19:33:10.958944   45976 version.go:256] remote version is much newer: v1.28.4; falling back to: stable-1.26 [upgrade/versions] Target version: v1.26.11 [upgrade/versions] Latest version in the v1.26 series: v1.26.11</p> <p>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT   CURRENT       TARGET kubelet     3 x v1.26.1   v1.26.11</p> <p>Upgrade to the latest version in the v1.26 series:</p> <p>COMPONENT                 CURRENT   TARGET kube-apiserver            v1.26.1   v1.26.11 kube-controller-manager   v1.26.1   v1.26.11 kube-scheduler            v1.26.1   v1.26.11 kube-proxy                v1.26.1   v1.26.11 CoreDNS                   v1.9.3    v1.9.3 etcd                      3.5.6-0   3.5.6-0</p> <p>You can now apply the upgrade by executing the following command:</p> <pre><code>    kubeadm upgrade apply v1.26.11\n</code></pre> <p>Note: Before you can perform this upgrade, you have to update kubeadm to v1.26.11.</p> <p>The table below shows the current state of component configs as understood by this version of kubeadm. Configs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually upgrade to is denoted in the \"PREFERRED VERSION\" column.</p> <p>API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no kubelet.config.k8s.io     v1beta1           v1beta1             no</p> <p>vagrant@controlplane-kubadm-vagrant:~$ sudo kubeadm upgrade apply v1.26.9 [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade/version] You have chosen to change the cluster version to \"v1.26.9\" [upgrade/versions] Cluster version: v1.26.1 [upgrade/versions] kubeadm version: v1.26.9 [upgrade] Are you sure you want to proceed? [y/N]: </p> <p>[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.26.9\". Enjoy!</p> <ul> <li>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</li> </ul> <pre><code>vagrant@controlplane-kubadm-vagrant:~$ kubectl drain controlplane-kubadm-vagrant --ignore-daemonsets\nnode/controlplane-kubadm-vagrant cordoned\nWarning: ignoring DaemonSet-managed Pods: calico-system/calico-node-lcbjx, calico-system/csi-node-driver-zv96f, kube-system/kube-proxy-9d7k4\nevicting pod tigera-operator/tigera-operator-54b47459dd-gwzf8\nevicting pod calico-system/calico-kube-controllers-6b7b9c649d-wgwht\nevicting pod calico-apiserver/calico-apiserver-7768c549f9-ftzfh\nevicting pod calico-apiserver/calico-apiserver-7768c549f9-xh2fr\nevicting pod kube-system/coredns-787d4945fb-7zv2b\nevicting pod calico-system/calico-typha-84fb845b64-gpxj9\nevicting pod kube-system/coredns-787d4945fb-qxc5k\npod/tigera-operator-54b47459dd-gwzf8 evicted\npod/calico-apiserver-7768c549f9-ftzfh evicted\npod/calico-kube-controllers-6b7b9c649d-wgwht evicted\npod/calico-apiserver-7768c549f9-xh2fr evicted\npod/coredns-787d4945fb-qxc5k evicted\npod/coredns-787d4945fb-7zv2b evicted\npod/calico-typha-84fb845b64-gpxj9 evicted\nnode/controlplane-kubadm-vagrant drained\n\nvagrant@controlplane-kubadm-vagrant:~$ kubectl get nodes\nNAME                          STATUS                     ROLES           AGE    VERSION\naux01-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\naux02-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\ncontrolplane-kubadm-vagrant   Ready,SchedulingDisabled   control-plane   5d4h   v1.26.1\n</code></pre> <ul> <li>Upgrade kubectl</li> </ul> <pre><code>apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.26.9-00' kubectl='1.26.9-00' &amp;&amp; \\\napt-mark hold kubelet kubectl\n</code></pre> <ul> <li>restart kubelet sudo systemctl daemon-reload sudo systemctl restart kubelet</li> </ul> <p>now are updated</p> <pre><code>vagrant@controlplane-kubadm-vagrant:~$ kubectl get nodes\nNAME                          STATUS                     ROLES           AGE    VERSION\naux01-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\naux02-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\ncontrolplane-kubadm-vagrant   Ready,SchedulingDisabled   control-plane   5d4h   v1.26.9\n</code></pre> <ul> <li>Bring the node back online by marking it schedulable:</li> </ul> <pre><code>vagrant@controlplane-kubadm-vagrant:~$ kubectl get nodes\nNAME                          STATUS   ROLES           AGE    VERSION\naux01-kubadm-vagrant          Ready    &lt;none&gt;          5d4h   v1.26.1\naux02-kubadm-vagrant          Ready    &lt;none&gt;          5d4h   v1.26.1\ncontrolplane-kubadm-vagrant   Ready    control-plane   5d4h   v1.26.9\n</code></pre>"},{"location":"cluster-architecture-installation-and-configuration/02.Actualizaci%C3%B3n%20-%20workers/","title":"02.Actualizaci\u00f3n   workers","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p> <p>apt-mark unhold kubeadm &amp;&amp; \\ apt-get update &amp;&amp; apt-get install -y kubeadm='1.27.x-*' &amp;&amp; \\ apt-mark hold kubeadm</p> <ul> <li>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</li> </ul> <pre><code>vagrant@controlplane-kubadm-vagrant:~$ kubectl drain aux01-kubadm-vagrant --ignore-daemonsets\nnode/controlplane-kubadm-vagrant cordoned\nWarning: ignoring DaemonSet-managed Pods: calico-system/calico-node-lcbjx, calico-system/csi-node-driver-zv96f, kube-system/kube-proxy-9d7k4\nevicting pod tigera-operator/tigera-operator-54b47459dd-gwzf8\nevicting pod calico-system/calico-kube-controllers-6b7b9c649d-wgwht\nevicting pod calico-apiserver/calico-apiserver-7768c549f9-ftzfh\nevicting pod calico-apiserver/calico-apiserver-7768c549f9-xh2fr\nevicting pod kube-system/coredns-787d4945fb-7zv2b\nevicting pod calico-system/calico-typha-84fb845b64-gpxj9\nevicting pod kube-system/coredns-787d4945fb-qxc5k\npod/tigera-operator-54b47459dd-gwzf8 evicted\npod/calico-apiserver-7768c549f9-ftzfh evicted\npod/calico-kube-controllers-6b7b9c649d-wgwht evicted\npod/calico-apiserver-7768c549f9-xh2fr evicted\npod/coredns-787d4945fb-qxc5k evicted\npod/coredns-787d4945fb-7zv2b evicted\npod/calico-typha-84fb845b64-gpxj9 evicted\nnode/controlplane-kubadm-vagrant drained\n\nvagrant@controlplane-kubadm-vagrant:~$ kubectl get nodes\nNAME                          STATUS                     ROLES           AGE    VERSION\naux01-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\naux02-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\ncontrolplane-kubadm-vagrant   Ready,SchedulingDisabled   control-plane   5d4h   v1.26.1\n</code></pre> <ul> <li>Upgrade kubectl</li> </ul> <pre><code>apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.26.9-00' kubectl='1.26.9-00' &amp;&amp; \\\napt-mark hold kubelet kubectl\n</code></pre> <ul> <li>restart kubelet sudo systemctl daemon-reload sudo systemctl restart kubelet</li> </ul> <p>now are updated</p> <pre><code>vagrant@controlplane-kubadm-vagrant:~$ kubectl get nodes\nNAME                          STATUS                     ROLES           AGE    VERSION\naux01-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\naux02-kubadm-vagrant          Ready                      &lt;none&gt;          5d4h   v1.26.1\ncontrolplane-kubadm-vagrant   Ready,SchedulingDisabled   control-plane   5d4h   v1.26.9\n</code></pre> <ul> <li>Bring the node back online by marking it schedulable:</li> </ul> <pre><code>vagrant@controlplane-kubadm-vagrant:~$ kubectl get nodes\nNAME                          STATUS   ROLES           AGE    VERSION\naux01-kubadm-vagrant          Ready    &lt;none&gt;          5d4h   v1.26.1\naux02-kubadm-vagrant          Ready    &lt;none&gt;          5d4h   v1.26.1\ncontrolplane-kubadm-vagrant   Ready    control-plane   5d4h   v1.26.9\n</code></pre>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/","title":"Backing up an etcd cluster","text":""},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#importance-of-etcd-backups","title":"Importance of etcd Backups","text":"<ol> <li>Cluster State Restoration: In case of data corruption, loss, or a disaster recovery scenario, an etcd backup allows you to restore the cluster's state to a previous point in time.</li> <li>Data Integrity and Consistency: Regular backups help ensure that you have a consistent snapshot of the cluster's state, which is crucial for troubleshooting and auditing purposes.</li> <li>Upgrade Rollbacks: When upgrading a Kubernetes cluster, having a recent etcd backup can be a lifesaver if you need to roll back to the previous version due to unforeseen issues.</li> </ol>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#how-to-back-up-an-etcd-cluster","title":"How to Back Up an etcd Cluster","text":"<p>The backup process for an etcd cluster can be done in several ways, but the most common method involves using the <code>etcdctl</code> command-line tool, which is a CLI for etcd. </p>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the etcd cluster: You need direct access to the etcd cluster. In the case of Kubernetes, this often means accessing the master nodes.</li> <li>etcdctl installed: This is the etcd command-line utility, often available on the master nodes of a Kubernetes cluster.</li> </ul>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#backup-process","title":"Backup Process","text":"<ul> <li>Search for the etcd pod and use the describe command to find the location of its certificates.:</li> </ul> <pre><code>controlplane $ kubectl get pods -A\nNAMESPACE            NAME                                      READY   STATUS    RESTARTS      AGE\n...\nkube-system          coredns-86b698fbb6-ww4kn                  1/1     Running   1 (38m ago)   13d\nkube-system          etcd-controlplane                         1/1     Running   2 (38m ago)   13d\nkube-system          kube-apiserver-controlplane               1/1     Running   2 (38m ago)   13d\nkube-system          kube-controller-manager-controlplane      1/1     Running   2 (38m ago)   13d\nkube-system          kube-proxy-f8kcp                          1/1     Running   2 (38m ago)   13d\n...\n</code></pre> <pre><code>kubectl describe po -n kube-system etcd-controlplane\n....\nContainers:\n  etcd:\n    Container ID:  containerd://66812357b67bfb2c7d5fa82d0d4c927dfee122cf8d8aac6089652cf7b2f7f972\n    Image:         registry.k8s.io/etcd:3.5.10-0\n    Image ID:      registry.k8s.io/etcd@sha256:22f892d7672adc0b9c86df67792afdb8b2dc08880f49f669eaaa59c47d7908c2\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Command:\n      etcd\n      --advertise-client-urls=https://172.30.1.2:2379\n      --cert-file=/etc/kubernetes/pki/etcd/server.crt\n      --client-cert-auth=true\n      --data-dir=/var/lib/etcd\n      --experimental-initial-corrupt-check=true\n      --experimental-watch-progress-notify-interval=5s\n      --initial-advertise-peer-urls=https://172.30.1.2:2380\n      --initial-cluster=controlplane=https://172.30.1.2:2380\n      --key-file=/etc/kubernetes/pki/etcd/server.key\n      --listen-client-urls=https://127.0.0.1:2379,https://172.30.1.2:2379\n      --listen-metrics-urls=http://127.0.0.1:2381\n      --listen-peer-urls=https://172.30.1.2:2380\n      --name=controlplane\n      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n      --peer-client-cert-auth=true\n      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n      --snapshot-count=10000\n      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n...\n</code></pre> <ul> <li>Take a Snapshot :</li> </ul> <p>This command saves a snapshot of the etcd store to a file. Ensure the path is secure and the file is transferred to a safe location.</p> <pre><code>ETCDCTL_API=3 etcdctl \\\n--cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \\\nsnapshot save backup.db\n</code></pre> <ul> <li>Verify the Snapshot (Optional but recommended):</li> </ul> <pre><code>etcdctl snapshot status backup.db\n</code></pre> <p>This command provides information about the snapshot, helping to verify its integrity and completeness.</p>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#when-to-back-up","title":"When to Back Up","text":"<ul> <li>Periodically: Depending on the cluster's workload and criticality, you might schedule backups hourly, daily, or weekly.</li> <li>Before Major Changes: Always take a backup before performing significant updates or changes to the cluster, such as upgrading Kubernetes or applying significant configuration changes.</li> </ul>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#automating-backups","title":"Automating Backups","text":"<p>For production environments, automate the backup process using cron jobs on the master node or through Kubernetes cron jobs that execute the backup process inside a pod with access to the etcd cluster.</p>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#restoration","title":"Restoration","text":"<p>To restore from a backup:</p> <pre><code>ETCDCTL_API=3 etcdctl snapshot restore backup.db\n</code></pre>"},{"location":"cluster-architecture-installation-and-configuration/backing-up-an-etcd-cluster/#additional-resources","title":"Additional Resources","text":"<p>For more detailed instructions and options, refer to the official etcd documentation on backup and restore: https://etcd.io/docs/v3.5/op-guide/recovery/</p> <p>For Kubernetes-specific guidance, the Kubernetes documentation provides insights into maintaining etcd, including backup strategies: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster </p> <p>Remember, the resilience of your Kubernetes cluster heavily relies on the health and integrity of your etcd backups. Regularly testing your backup and restoration process is crucial to ensure your disaster recovery strategy is effective.</p>"},{"location":"cluster-architecture-installation-and-configuration/certificate-signing-requests-normal-user/","title":"Certificate Signing Requests (Normal User)","text":"<p>To manage Certificate Signing Requests (CSRs) for a normal user in Kubernetes, it's crucial to first understand what a CSR is and how it functions within the context of Kubernetes. A CSR is a request sent from an applicant to a Certificate Authority (CA) to obtain a digital certificate. In Kubernetes, this process is used for authentication and the creation of certificates that enable secure communication between users and services within the cluster.</p> <p>Here's a step-by-step example on how to generate a CSR for a normal user and submit it to Kubernetes for signing, which is a common task for administrators preparing the environment for users or services.</p>"},{"location":"cluster-architecture-installation-and-configuration/certificate-signing-requests-normal-user/#step-1-create-a-configuration-file-for-the-user","title":"Step 1: Create a configuration file for the user","text":"<p>First, you need to generate a key pair (public and private) for the user. We'll use <code>openssl</code> for this purpose. Ensure you have <code>openssl</code> installed on your machine.</p> <pre><code>openssl genrsa -out user.key 2048\n</code></pre> <p>Next, create a CSR using the user's private key. During this step, you will specify the username and the group the user will belong to in the <code>subject</code> field of the CSR.</p> <pre><code>openssl req -new -key user.key -out user.csr -subj \"/CN=user/O=group\"\n</code></pre> <p>Where <code>user</code> is the username, and <code>group</code> is the group the user will belong to.</p>"},{"location":"cluster-architecture-installation-and-configuration/certificate-signing-requests-normal-user/#step-2-create-the-csr-in-kubernetes","title":"Step 2: Create the CSR in Kubernetes","text":"<p>Before you can submit the CSR to Kubernetes, you need to encode it in base64 and create a CSR manifest in Kubernetes.</p> <pre><code>cat user.csr | base64 | tr -d \"\\n\"\n</code></pre> <p>Create a YAML file (<code>user-csr.yaml</code>) with the content of the encoded base64 CSR:</p> <pre><code>apiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: user-request\nspec:\n  request: &lt;YOUR-BASE64-ENCODED-CSR-HERE&gt;\n  signerName: kubernetes.io/kube-apiserver-client\n  usages:\n  - client auth\n</code></pre> <p>Replace <code>&lt;YOUR-BASE64-ENCODED-CSR-HERE&gt;</code> with your base64 encoded CSR.</p> <p>Apply this file to your Kubernetes cluster:</p> <pre><code>kubectl apply -f user-csr.yaml\n</code></pre>"},{"location":"cluster-architecture-installation-and-configuration/certificate-signing-requests-normal-user/#step-3-approve-the-csr","title":"Step 3: Approve the CSR","text":"<p>Once the CSR is submitted, you need to manually approve it for Kubernetes to issue the certificate:</p> <pre><code>kubectl certificate approve user-request\n</code></pre>"},{"location":"cluster-architecture-installation-and-configuration/certificate-signing-requests-normal-user/#step-4-retrieve-the-certificate","title":"Step 4: Retrieve the Certificate","text":"<p>After the CSR is approved, you can fetch the issued certificate:</p> <pre><code>kubectl get csr user-request -o jsonpath='{.status.certificate}' | base64 --decode &gt; user.crt\n</code></pre>"},{"location":"cluster-architecture-installation-and-configuration/certificate-signing-requests-normal-user/#step-5-use-the-certificate","title":"Step 5: Use the Certificate","text":"<p>Now that you have the certificate (<code>user.crt</code>) and the private key (<code>user.key</code>), you can configure your Kubernetes client (<code>kubectl</code>) to use these credentials to authenticate as the user.</p> <p>This process allows you to securely manage identities within your Kubernetes cluster, using the cluster's native authentication mechanisms.</p> <p>For more details and best practices on managing certificates and CSRs in Kubernetes, consult the official documentation: https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</p> <p>This procedure is essential for administrators and users who need to configure secure access to a Kubernetes cluster, being a crucial skill for the CKA exam.</p>"},{"location":"commands/commands/","title":"Comandos Imperativos (Imperative Commands)","text":"<p>Los comandos imperativos son una forma r\u00e1pida y directa de realizar tareas en Kubernetes sin necesidad de definir archivos de configuraci\u00f3n YAML.</p> <ol> <li> <p>Crear un Pod de manera imperativa:</p> <ul> <li>Comando: <code>kubectl run [nombre_del_pod] --image=[nombre_de_la_imagen]</code></li> <li>Prop\u00f3sito: Crea un nuevo pod de manera imperativa con el nombre y la imagen especificados.</li> </ul> </li> <li> <p>Exponer un servicio de manera imperativa:</p> <ul> <li>Comando: <code>kubectl expose pod [nombre_del_pod] --port=[puerto] --target-port=[puerto_destino] --name=[nombre_del_servicio]</code></li> <li>Prop\u00f3sito: Expone un servicio de manera imperativa para un pod espec\u00edfico con los puertos especificados.</li> </ul> </li> <li> <p>Escalado de replicaci\u00f3n de un Deployment de manera imperativa:</p> <ul> <li>Comando: <code>kubectl scale deployment [nombre_del_despliegue] --replicas=[n\u00famero_de_replicas]</code></li> <li>Prop\u00f3sito: Realiza el escalado de replicaci\u00f3n de un deployment de manera imperativa ajustando el n\u00famero de r\u00e9plicas.</li> </ul> </li> <li> <p>Creaci\u00f3n de un espacio de nombres de manera imperativa:</p> <ul> <li>Comando: <code>kubectl create namespace [nombre_del_namespace]</code></li> <li>Prop\u00f3sito: Crea un nuevo espacio de nombres de manera imperativa.</li> </ul> </li> <li> <p>Establecer el espacio de nombres por defecto para el contexto actual de manera imperativa:</p> <ul> <li>Comando: <code>kubectl config set-context --current --namespace=[nombre_del_namespace]</code></li> <li>Prop\u00f3sito: Establece de manera imperativa el espacio de nombres por defecto para el contexto actual.</li> </ul> </li> <li> <p>Creaci\u00f3n de un servicio de manera imperativa:</p> <ul> <li>Comando: <code>kubectl create service clusterip [nombre_del_servicio] --tcp=[puerto]:[puerto_destino] --dry-run=client -o yaml &gt; service.yaml</code></li> <li>Prop\u00f3sito: Crea un servicio de manera imperativa y genera la definici\u00f3n YAML para el servicio.</li> </ul> </li> <li> <p>Eliminaci\u00f3n de un recurso de manera imperativa:</p> <ul> <li>Comando: <code>kubectl delete [tipo_de_recurso] [nombre_del_recurso]</code></li> <li>Prop\u00f3sito: Elimina un recurso espec\u00edfico de manera imperativa.</li> </ul> </li> <li> <p>Obtenci\u00f3n de la configuraci\u00f3n YAML de un recurso de manera imperativa:</p> <ul> <li>Comando: <code>kubectl get [tipo_de_recurso] [nombre_del_recurso] -o yaml &gt; resource.yaml</code></li> <li>Prop\u00f3sito: Obtiene de manera imperativa la configuraci\u00f3n YAML de un recurso espec\u00edfico.</li> </ul> </li> <li> <p>Actualizaci\u00f3n de un recurso de manera imperativa:</p> <ul> <li>Comando: <code>kubectl set [tipo_de_recurso] [nombre_del_recurso] --[atributo]=[nuevo_valor]</code></li> <li>Prop\u00f3sito: Actualiza de manera imperativa un atributo espec\u00edfico de un recurso.</li> </ul> </li> </ol>"},{"location":"commands/commands/#comandos-para-trabajar-con-nodos-nodes","title":"Comandos para trabajar con Nodos (Nodes)","text":"<ol> <li> <p>Obtener informaci\u00f3n sobre los nodos:</p> <ul> <li>Comando: <code>kubectl get nodes</code></li> <li>Prop\u00f3sito: Muestra una lista de todos los nodos en el cl\u00faster.</li> </ul> </li> <li> <p>Describir un nodo espec\u00edfico:</p> <ul> <li>Comando: <code>kubectl describe node [nombre_del_nodo]</code></li> <li>Prop\u00f3sito: Proporciona informaci\u00f3n detallada sobre un nodo espec\u00edfico, incluyendo detalles sobre recursos, capacidad y estado.</li> </ul> </li> </ol>"},{"location":"commands/commands/#comandos-para-trabajar-con-espacios-de-nombres-namespaces","title":"Comandos para trabajar con Espacios de Nombres (Namespaces)","text":"<ol> <li> <p>Crear un nuevo espacio de nombres:</p> <ul> <li>Comando: <code>kubectl create namespace [nombre_del_namespace]</code></li> <li>Prop\u00f3sito: Crea un nuevo espacio de nombres en el cl\u00faster.</li> </ul> </li> <li> <p>Establecer el espacio de nombres por defecto para el contexto actual:</p> <ul> <li>Comando: <code>kubectl config set-context --current --namespace=[nombre_del_namespace]</code></li> <li>Prop\u00f3sito: Establece el espacio de nombres por defecto para el contexto actual. Los comandos subsiguientes operar\u00e1n en este espacio de nombres.</li> </ul> </li> <li> <p>Obtener la configuraci\u00f3n del espacio de nombres y guardarla en un archivo YAML:</p> <ul> <li>Comando: <code>kubectl get namespace [nombre_del_namespace] -o yaml &gt; [nombre_del_archivo_yaml]</code></li> <li>Prop\u00f3sito: Obtiene la configuraci\u00f3n del espacio de nombres especificado y guarda la informaci\u00f3n en un archivo YAML llamado team-12-namespace.yml.</li> </ul> </li> </ol>"},{"location":"commands/commands/#comandos-para-trabajar-con-pods","title":"Comandos para trabajar con Pods","text":"<ol> <li> <p>Obtener informaci\u00f3n sobre los pods:</p> <ul> <li>Comando: <code>kubectl get pods</code></li> <li>Prop\u00f3sito: Muestra una lista de todos los pods en el cl\u00faster.</li> </ul> </li> <li> <p>Crear un nuevo pod:</p> <ul> <li>Comando: <code>kubectl run [nombre_del_pod] --image=[nombre_de_la_imagen]</code></li> <li>Prop\u00f3sito: Crea un nuevo pod con el nombre y la imagen especificados.</li> </ul> </li> <li> <p>Obtener detalles de un pod:</p> <ul> <li>Comando: <code>kubectl describe pod [nombre_del_pod]</code></li> <li>Prop\u00f3sito: Proporciona informaci\u00f3n detallada sobre un pod espec\u00edfico, incluida la imagen utilizada.</li> </ul> </li> <li> <p>Obtener informaci\u00f3n detallada sobre los nodos en los que se ejecutan los pods:</p> <ul> <li>Comando: <code>kubectl get pods -o wide</code></li> <li>Prop\u00f3sito: Muestra informaci\u00f3n adicional, incluido el nodo en el que se ejecuta cada pod.</li> </ul> </li> <li> <p>Eliminar un pod:</p> <ul> <li>Comando: <code>kubectl delete pod [nombre_del_pod]</code></li> <li>Prop\u00f3sito: Elimina un pod espec\u00edfico.</li> </ul> </li> <li> <p>Editar un pod para cambiar la imagen:</p> <ul> <li>Comando: <code>kubectl edit pod [nombre_del_pod]</code></li> <li>Prop\u00f3sito: Abre un editor para modificar la definici\u00f3n de un pod y realizar cambios, como cambiar la imagen utilizada.</li> </ul> </li> <li> <p>Reemplazar un recurso existente con un nuevo recurso proporcionado.</p> <ul> <li>Comando: <code>kubectl replace --force -f [nombre_del_archivo_yaml]</code></li> <li>Prop\u00f3sito: El comando kubectl replace se utiliza para actualizar un recurso existente en el cl\u00faster de Kubernetes con la definici\u00f3n de un nuevo recurso proporcionado en un archivo YAML. La opci\u00f3n --force indica que se debe realizar el reemplazo sin solicitar confirmaci\u00f3n adicional.</li> </ul> </li> </ol>"},{"location":"commands/commands/#comandos-para-trabajar-con-replicasets","title":"Comandos para trabajar con ReplicaSets","text":"<ol> <li> <p>Obtener informaci\u00f3n sobre los ReplicaSets:</p> <ul> <li>Comando: <code>kubectl get ReplicaSet</code></li> <li>Prop\u00f3sito: Muestra una lista de todos los ReplicaSets en el cl\u00faster.</li> </ul> </li> <li> <p>Crear un ReplicaSet:</p> <ul> <li>Comando: <code>kubectl create -f [nombre_del_archivo_yaml]</code></li> <li>Prop\u00f3sito: Crea un ReplicaSet utilizando un archivo de definici\u00f3n YAML.</li> </ul> </li> <li> <p>Obtener detalles de un ReplicaSet:</p> <ul> <li>Comando: <code>kubectl describe ReplicaSet [nombre_del_ReplicaSet]</code></li> <li>Prop\u00f3sito: Proporciona informaci\u00f3n detallada sobre un ReplicaSet espec\u00edfico, incluyendo la imagen utilizada para crear los pods.</li> </ul> </li> <li> <p>Eliminar un pod:</p> <ul> <li>Comando: <code>kubectl delete pod [nombre_del_pod]</code></li> <li>Prop\u00f3sito: Elimina un pod espec\u00edfico.</li> </ul> </li> <li> <p>Escalar un ReplicaSet a cinco pods:</p> <ul> <li>Comando: <code>kubectl scale --replicas=5 replicaset [nombre_del_ReplicaSet]</code></li> <li>Prop\u00f3sito: Escala un ReplicaSet para que tenga cinco pods en funcionamiento.</li> </ul> </li> <li> <p>Escalar un ReplicaSet a dos pods:</p> <ul> <li>Comando: <code>kubectl edit replicaset [nombre_del_ReplicaSet]</code></li> <li>Prop\u00f3sito: Edita un ReplicaSet y ajusta el n\u00famero de r\u00e9plicas a dos.</li> </ul> </li> </ol>"},{"location":"commands/commands/#comandos-para-trabajar-con-despliegues-deployments","title":"Comandos para trabajar con Despliegues (Deployments)","text":"<ol> <li> <p>Verificar los despliegues existentes:</p> <ul> <li>Comando: <code>kubectl get deployments</code></li> <li>Prop\u00f3sito: Muestra una lista de todos los despliegues en el cl\u00faster.</li> </ul> </li> <li> <p>Crear un nuevo despliegue:</p> <ul> <li>Comando: <code>kubectl create deployment [nombre] --image=[nombre_de_la_imagen] --replicas=[n\u00famero_de_replicas]</code></li> <li>Prop\u00f3sito: Crea un nuevo despliegue con el nombre, la imagen y el n\u00famero de r\u00e9plicas especificados.</li> </ul> </li> </ol>"},{"location":"commands/commands/#comandos-para-trabajar-con-services","title":"Comandos para trabajar con Services","text":"<ol> <li> <p>Obtener informaci\u00f3n sobre los servicios:</p> <ul> <li>Comando: <code>kubectl get service</code> o <code>kubectl get svc</code></li> <li>Prop\u00f3sito: Muestra una lista de todos los servicios en el cl\u00faster.</li> </ul> </li> <li> <p>Obtener el tipo del servicio predeterminado de Kubernetes:</p> <ul> <li>Comando: <code>kubectl get service &lt;nombre_del_servicio&gt; -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Obtiene el tipo del servicio de Kubernetes en un namespace espec\u00edfico.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es el puerto de destino configurado en el servicio de Kubernetes?</p> <ul> <li>Comando: <code>kubectl describe service &lt;nombre_del_servicio&gt; -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Muestra informaci\u00f3n detallada del servicio, incluyendo el puerto de destino.</li> </ul> </li> <li> <p>\u00bfCu\u00e1ntas etiquetas est\u00e1n configuradas en el servicio de Kubernetes?</p> <ul> <li>Comando: <code>kubectl describe service &lt;nombre_del_servicio&gt; -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Muestra el n\u00famero de etiquetas configuradas en el servicio.</li> </ul> </li> <li> <p>\u00bfCu\u00e1ntos puntos finales est\u00e1n conectados al servicio de Kubernetes?</p> <ul> <li>Comando: <code>kubectl describe service &lt;nombre_del_servicio&gt; -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Muestra el n\u00famero de puntos finales conectados al servicio.</li> </ul> </li> <li> <p>\u00bfCu\u00e1ntos despliegues existen en el espacio de nombres predeterminado actual?</p> <ul> <li>Comando: <code>kubectl get deployments -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Obtiene el n\u00famero de despliegues en un namespace espec\u00edfico.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es la imagen utilizada para crear las c\u00e1psulas en el despliegue?</p> <ul> <li>Comando: <code>kubectl describe deployment &lt;nombre_del_despliegue&gt; -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Muestra la imagen utilizada para crear las c\u00e1psulas en un despliegue espec\u00edfico.</li> </ul> </li> <li> <p>\u00bfPuedes acceder a la interfaz de la aplicaci\u00f3n web?</p> <ul> <li>Prop\u00f3sito: Verifica si puedes acceder a la interfaz de la aplicaci\u00f3n web.</li> </ul> </li> <li> <p>\u00bfQu\u00e9 se necesita hacer para acceder a la interfaz de la aplicaci\u00f3n web?</p> <ul> <li>Comando: <code>kubectl apply -f service-definition.yaml</code></li> <li>Prop\u00f3sito: Crea un nuevo servicio para acceder a la interfaz de la aplicaci\u00f3n web.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es el nombre del nuevo servicio creado para acceder a la interfaz de la aplicaci\u00f3n web?</p> <ul> <li>Comando: <code>kubectl get service</code></li> <li>Prop\u00f3sito: Obtiene el nombre del nuevo servicio creado para acceder a la interfaz de la aplicaci\u00f3n web.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es el tipo del nuevo servicio?</p> <ul> <li>Comando: <code>kubectl describe service web-app-service -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Obtiene el tipo del nuevo servicio creado.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es el puerto de destino del nuevo servicio?</p> <ul> <li>Comando: <code>kubectl describe service web-app-service -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Obtiene el puerto de destino del nuevo servicio.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es el puerto del nuevo servicio?</p> <ul> <li>Comando: <code>kubectl describe service web-app-service -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Obtiene el puerto del nuevo servicio.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es el puerto de nodo del nuevo servicio?</p> <ul> <li>Comando: <code>kubectl describe service web-app-service -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Obtiene el puerto de nodo del nuevo servicio.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es el nombre del selector del nuevo servicio?</p> <ul> <li>Comando: <code>kubectl describe service web-app-service -n &lt;nombre_del_namespace&gt;</code></li> <li>Prop\u00f3sito: Obtiene el nombre del selector del nuevo servicio.</li> </ul> </li> </ol>"},{"location":"commands/services/","title":"Services","text":"<p>Configuraci\u00f3n y Creaci\u00f3n de Recursos</p> <p>Crear un Deployment en Kubernetes:</p> <pre><code>kubectl create deployment &lt;nombre&gt; --image=&lt;imagen&gt;\n</code></pre> <p>Crear un Deployment con replicas espec\u00edficas:</p> <pre><code>kubectl create deployment &lt;nombre&gt; --replicas=&lt;n\u00famero&gt; --image=&lt;imagen&gt;\n</code></pre> <p>Actualizar la imagen de un Deployment:</p> <pre><code>kubectl set image deployment/&lt;nombre&gt; &lt;nombre&gt;=&lt;nueva-imagen&gt;\n</code></pre> <p>Exponer un Deployment como un servicio:</p> <pre><code>kubectl expose deployment &lt;nombre&gt; --type=NodePort --port=&lt;puerto&gt;\n</code></pre> <p>Gesti\u00f3n de Recursos</p> <p>Escalar un Deployment:</p> <pre><code>kubectl scale deployment &lt;nombre&gt; --replicas=&lt;n\u00famero&gt;\n</code></pre> <p>Eliminar un Deployment:</p> <pre><code>kubectl delete deployment &lt;nombre&gt;\n</code></pre> <p>Ver registros de un Pod:</p> <pre><code>kubectl logs -l &lt;selector&gt;\n</code></pre> <p>Ejecutar un comando en un Pod:</p> <pre><code>kubectl exec -it &lt;nombre-del-pod&gt; -- &lt;comando&gt;\n</code></pre> <p>Copiar archivos desde o hacia un Pod:</p> <pre><code>kubectl cp &lt;nombre-del-pod&gt;:&lt;ruta-remota&gt; &lt;ruta-local&gt;\n</code></pre> <p>Ver y Depurar Recursos</p> <p>Ver la informaci\u00f3n detallada de un recurso:</p> <pre><code>kubectl describe &lt;recurso&gt; &lt;nombre&gt;\n</code></pre> <p>Ver los eventos relacionados con un recurso:</p> <pre><code>kubectl get events\n</code></pre> <p>Ejecutar una shell dentro de un Pod:</p> <pre><code>kubectl exec -it &lt;nombre-del-pod&gt; -- /bin/sh\n</code></pre> <p>Ver la configuraci\u00f3n de un recurso en YAML:</p> <pre><code>kubectl get &lt;recurso&gt; &lt;nombre&gt; -o yaml\n</code></pre> <p>Gesti\u00f3n de Estado</p> <p>Reiniciar un Pod:</p> <pre><code>kubectl delete pod &lt;nombre-del-pod&gt;\n</code></pre> <p>Eliminar todos los Pods de un Deployment:</p> <pre><code>kubectl delete pods -l &lt;selector&gt;\n</code></pre> <p>Forzar la terminaci\u00f3n de un Pod:</p> <pre><code>kubectl delete pod &lt;nombre-del-pod&gt; --grace-period=0 --force\n</code></pre> <p>Desencadenar un Deployment manualmente:</p> <pre><code>kubectl rollout restart deployment/&lt;nombre-del-deployment&gt;\n</code></pre> <p>Escalado y Balanceo de Carga</p> <p>Escalar un Deployment manualmente:</p> <pre><code>kubectl scale deployment &lt;nombre&gt; --replicas=&lt;n\u00famero&gt;\n</code></pre> <p>Exponer un servicio en un puerto espec\u00edfico:</p> <pre><code>kubectl expose deployment &lt;nombre&gt; --port=&lt;puerto&gt;\n</code></pre> <p>Crear un servicio de tipo LoadBalancer:</p> <pre><code>kubectl expose deployment &lt;nombre&gt; --type=LoadBalancer --port=&lt;puerto&gt;\n</code></pre> <p>Configurar el balanceo de carga para un servicio:</p> <pre><code>kubectl scale deployment/&lt;nombre-del-deployment&gt; --replicas=&lt;n\u00famero&gt;\n</code></pre> <p>Control de Acceso y Seguridad</p> <p>Crear un usuario y asignarle un rol:</p> <pre><code>kubectl create serviceaccount &lt;nombre-del-usuario&gt;\nkubectl create clusterrolebinding &lt;nombre-del-binding&gt; --clusterrole=&lt;nombre-del-rol&gt; --serviceaccount=&lt;nombre-del-namespace&gt;:&lt;nombre-del-usuario&gt;\n</code></pre> <p>Crear un secreto a partir de un archivo:</p> <pre><code>kubectl create secret generic &lt;nombre-del-secreto&gt; --from-file=&lt;clave&gt;=&lt;archivo&gt;\n</code></pre> <p>Crear una regla de red para permitir el tr\u00e1fico entrante:</p> <pre><code>kubectl create networkpolicy allow-traffic --pod-selector=&lt;selector-del-pod&gt; --namespace=&lt;nombre-del-namespace&gt; --ingress --port=&lt;puerto&gt;\n</code></pre> <p>Obtener informaci\u00f3n de un recurso concreto:</p> <pre><code>kubectl get &lt;recurso&gt; &lt;nombre&gt; -o json\n</code></pre>"},{"location":"core-concepts/00.control-plane/","title":"Control Plane de Kubernetes","text":"<p>El control plane de Kubernetes es un conjunto de componentes esenciales que trabajan en conjunto para gestionar y controlar el estado del cl\u00faster. Estos componentes garantizan la orquestaci\u00f3n eficiente de los recursos y la ejecuci\u00f3n de aplicaciones en contenedores. A continuaci\u00f3n, se describen los principales componentes del control plane:</p>"},{"location":"core-concepts/00.control-plane/#kube-apiserver","title":"<code>kube-apiserver</code>","text":"<p>El <code>kube-apiserver</code> es el punto de entrada para el control plane. Es responsable de exponer la API de Kubernetes y validar/configurar datos antes de almacenarlos en el almac\u00e9n de datos persistente.</p>"},{"location":"core-concepts/00.control-plane/#etcd","title":"<code>etcd</code>","text":"<p><code>etcd</code> es un almac\u00e9n de datos distribuido que mantiene el estado del cl\u00faster y la configuraci\u00f3n. El <code>kube-apiserver</code> interact\u00faa con <code>etcd</code> para almacenar y recuperar datos cr\u00edticos del cl\u00faster.</p>"},{"location":"core-concepts/00.control-plane/#kube-controllers","title":"kube-controllers","text":"<p>El <code>kube-controllers</code> aloja varios controladores que observan el estado del cl\u00faster a trav\u00e9s de la API del servidor. Estos controladores toman acciones para garantizar que el estado del cl\u00faster coincida con el estado deseado, ejecutando tareas como la gesti\u00f3n de replicaset y el escalado autom\u00e1tico.</p>"},{"location":"core-concepts/00.control-plane/#kube-scheduler","title":"kube-scheduler","text":"<p>El <code>kube-scheduler</code> es responsable de tomar decisiones sobre en qu\u00e9 nodos se deben ejecutar los Pods reci\u00e9n creados. Eval\u00faa diversas restricciones y la disponibilidad de recursos para realizar asignaciones eficientes.</p>"},{"location":"core-concepts/00.control-plane/#conclusiones","title":"Conclusiones","text":"<p>El control plane de Kubernetes es esencial para la gesti\u00f3n efectiva de cl\u00fasteres de contenedores. Cada componente desempe\u00f1a un papel crucial en garantizar la disponibilidad, escalabilidad y confiabilidad de las aplicaciones implementadas en Kubernetes.</p> <p>Para obtener m\u00e1s detalles sobre cada componente, consulte la documentaci\u00f3n oficial de Kubernetes.</p>"},{"location":"core-concepts/etcd/","title":"etcd","text":""},{"location":"core-concepts/etcd/#introduccion","title":"Introducci\u00f3n","text":"<p><code>etcd</code> es un almac\u00e9n de clave-valor distribuido utilizado en Kubernetes para almacenar configuraciones y metadatos cr\u00edticos. <code>etcd</code> desempe\u00f1a un papel fundamental al proporcionar un almacenamiento distribuido confiable, consistente y coordinado, lo que contribuye a la estabilidad, confiabilidad y escalabilidad de los cl\u00fasteres de Kubernetes.</p>"},{"location":"core-concepts/etcd/#inicio-rapido-con-etcd","title":"Inicio R\u00e1pido con etcd","text":"<p>Kubeadm implementa etcd como un pod en el espacio de nombres kube-system. No obstante, si deseas probarlo de manera independiente, puedes realizar la siguiente prueba:</p> <pre><code>rm -rf /tmp/etcd-data.tmp &amp;&amp; mkdir -p /tmp/etcd-data.tmp &amp;&amp; \\\n  docker rmi gcr.io/etcd-development/etcd:v3.5.10 || true &amp;&amp; \\\n  docker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --mount type=bind,source=/tmp/etcd-data.tmp,destination=/etcd-data \\\n  --name etcd-gcr-v3.5.10 \\\n  gcr.io/etcd-development/etcd:v3.5.10 \\\n  /usr/local/bin/etcd \\\n  --name s1 \\\n  --data-dir /etcd-data \\\n  --listen-client-urls http://0.0.0.0:2379 \\\n  --advertise-client-urls http://0.0.0.0:2379 \\\n  --listen-peer-urls http://0.0.0.0:2380 \\\n  --initial-advertise-peer-urls http://0.0.0.0:2380 \\\n  --initial-cluster s1=http://0.0.0.0:2380 \\\n  --initial-cluster-token tkn \\\n  --initial-cluster-state new \\\n  --log-level info \\\n  --logger zap \\\n  --log-outputs stderr\n\ndocker exec etcd-gcr-v3.5.10 /usr/local/bin/etcd --version\ndocker exec etcd-gcr-v3.5.10 /usr/local/bin/etcdctl version\ndocker exec etcd-gcr-v3.5.10 /usr/local/bin/etcdutl version\ndocker exec etcd-gcr-v3.5.10 /usr/local/bin/etcdctl endpoint health\ndocker exec etcd-gcr-v3.5.10 /usr/local/bin/etcdctl put foo bar\ndocker exec etcd-gcr-v3.5.10 /usr/local/bin/etcdctl get foo\n</code></pre> <p>M\u00e1s informaci\u00f3n: https://github.com/etcd-io/etcd/releases/tag/v3.5.10</p>"},{"location":"core-concepts/kube-apiserver/","title":"kube-apiserver","text":""},{"location":"core-concepts/kube-apiserver/#introduccion","title":"Introducci\u00f3n","text":"<p>El kube-apiserver es crucial para gestionar cambios en el cl\u00faster. Autentica, valida solicitudes y actualiza datos en etcd, siendo el \u00fanico componente que se comunica directamente con \u00e9ste. Otros componentes como el kube-scheduler, kube-controller-manager y kubelet utilizan el kube-apiserver para realizar actualizaciones en sus \u00e1reas espec\u00edficas del cl\u00faster.</p>"},{"location":"core-concepts/kube-apiserver/#funciones-del-kube-apiserver","title":"Funciones del kube-apiserver","text":"<p>El kube-apiserver cumple diversas funciones cr\u00edticas en Kubernetes:</p> <ul> <li> <p>Autenticaci\u00f3n y autorizaci\u00f3n: Verifica la identidad de los usuarios y componentes que intentan interactuar con el cl\u00faster, y decide qu\u00e9 acciones est\u00e1n permitidas.</p> </li> <li> <p>Exposici\u00f3n de la API: Ofrece una interfaz RESTful que permite a los usuarios y aplicaciones acceder a los recursos y funcionalidades del cl\u00faster.</p> </li> <li> <p>Validaci\u00f3n y admisi\u00f3n: Examina y valida las solicitudes de API entrantes para garantizar que cumplen con las pol\u00edticas y restricciones del cl\u00faster.</p> </li> <li> <p>Persistencia de recursos: Almacena y recupera la informaci\u00f3n sobre el estado del cl\u00faster en el almac\u00e9n de datos etcd.</p> </li> <li> <p>Notificaciones de eventos: Genera eventos para informar sobre cambios en el estado del cl\u00faster.</p> </li> </ul>"},{"location":"core-concepts/kube-controllers/","title":"kube-controllers","text":"<p>En Kubernetes, los<code>controller-manager</code> son componentes esenciales en Kubernetes que garantizan que el estado del cl\u00faster se mantenga de acuerdo con las especificaciones deseadas. A trav\u00e9s de sus bucles de control, supervisan, ajustan y mantienen los recursos del cl\u00faster, lo que facilita la administraci\u00f3n y la orquestaci\u00f3n de aplicaciones en un entorno de Kubernetes. </p>"},{"location":"core-concepts/kube-controllers/#tipos-de-controladores-en-kubernetes","title":"Tipos de Controladores en Kubernetes","text":""},{"location":"core-concepts/kube-controllers/#replicaset","title":"Replicaset","text":"<ul> <li>Garantiza un n\u00famero espec\u00edfico de Pods en ejecuci\u00f3n.</li> <li>Elimina o crea Pods seg\u00fan sea necesario.</li> </ul>"},{"location":"core-concepts/kube-controllers/#deployment","title":"Deployment","text":"<ul> <li>Ejecuta un Pod con un n\u00famero deseado de r\u00e9plicas.</li> <li>Permite estrategias de implementaci\u00f3n, como actualizaciones sin tiempo de inactividad.</li> </ul>"},{"location":"core-concepts/kube-controllers/#daemonset","title":"Daemonset","text":"<ul> <li>Garantiza que todos o algunos nodos ejecuten una copia del Pod.</li> <li>\u00datil para implementar un Pod por nodo o por subconjunto de nodos.</li> </ul>"},{"location":"core-concepts/kube-controllers/#statefulset","title":"Statefulset","text":"<ul> <li>Adecuado para cargas de trabajo con almacenamiento persistente.</li> <li>Mantiene identidades \u00fanicas para cada Pod y gestiona eventos del ciclo de vida.</li> </ul>"},{"location":"core-concepts/kube-controllers/#job","title":"Job","text":"<ul> <li>Supervisa Pods para tareas espec\u00edficas.</li> <li>Utilizado para procesamiento por lotes.</li> <li>Los Pods no se eliminan autom\u00e1ticamente; debe hacerse manualmente.</li> </ul>"},{"location":"core-concepts/kube-controllers/#cronjob","title":"Cronjob","text":"<ul> <li>Similar a Job, pero se ejecuta seg\u00fan un horario definido.</li> <li>Gestiona autom\u00e1ticamente la creaci\u00f3n de Jobs seg\u00fan la programaci\u00f3n.</li> </ul>"},{"location":"core-concepts/kube-controllers/#controladores-personalizados","title":"Controladores Personalizados","text":"<ul> <li>Extienden la funcionalidad de Kubernetes.</li> <li>Permite a los desarrolladores crear comportamientos personalizados.</li> <li>Ejemplo: <code>kubernetes-external-secrets</code> para gestionar secretos externos.</li> </ul>"},{"location":"core-concepts/kube-controllers/#escritura-de-controladores-personalizados","title":"Escritura de Controladores Personalizados","text":"<ul> <li>No es necesario usar Go; se pueden usar varios lenguajes.</li> <li>Ejemplo: <code>kubernetes-external-secrets</code> est\u00e1 escrito en JavaScript.</li> <li>Monitorizan recursos y act\u00faan en cambios detectados.</li> </ul>"},{"location":"quiz/quiz2/","title":"Quiz - 2","text":"<ol> <li>List all pods in the current namespace, with more details</li> <li>List Events sorted by timestamp</li> <li>List all pods in all namespaces</li> <li>Rolling restart of the \u201cfrontend\u201d deployment</li> <li>Show Merged kubeconfig settings</li> <li>Get the documentation for pod manifests</li> <li>Get all running pods in the namespace</li> <li>Start a single instance of nginx</li> <li>Rolling update \u201cwww\u201d containers of \u201cfrontend\u201d deployment, updating the image</li> <li>List pods Sorted by Restart Count</li> <li>Create a daemonset named \u201cPrometheus-monitoring\u201d using image=prom/Prometheus which runs in all the nodes in the cluster.</li> </ol> <pre><code>TODO\n</code></pre> <ol> <li>Get the deployment <code>nginx-deployment</code>rollout status </li> </ol> <pre><code>kubectl rollout history deployment/nginx-deployment\n</code></pre> <ol> <li>Print pod name and start time to \u201c/opt/pod-status\u201d file</li> </ol> <pre><code>k get pod -A -o custom-columns=NAME:.metadata.name,START:.status.startTime&gt;/opt/pod-status\n\nkubectl get pod  -A -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.startTime}{\"\\n\"}{end}'\n\n</code></pre> <ol> <li>Undo the deployment to the previous version 1.17.1 and verify Image has the previous version</li> </ol> <pre><code>kubectl rollout undo deployment/deployment-nginx\n</code></pre> <ol> <li>Create a busybox pod and add \u201csleep 3600\u201d command</li> <li>Resume the rollout of the deployment</li> <li>Fix a node that shows as non-ready</li> <li>List all the pods showing name and namespace with a json path expression</li> <li>Scale down the deployment to 1 replica</li> <li>Create 5 nginx pods in which two of them is labeled env=prod and three of them is labeled env=dev</li> <li>List all the pods that are serviced by the service \u201cwebservice\u201d and copy the output in /opt/$USER/webservice.targets</li> <li>Get list of all pods in all namespaces and write it to file \u201c/opt/pods-list.yaml\u201d</li> <li> <p>Create an nginx pod which loads the secret as environment variables</p> </li> <li> <p>Update the deployment with the image version 1.16.1 and verify the image and check the rollout history</p> </li> <li> <p>Create a deployment called webapp with image nginx having 5 replicas in it, put the file in /tmp directory with named webapp.yaml</p> </li> <li> <p>Evict all existing pods from a node-1 and make the node unschedulable for new pods.</p> </li> <li> <p>Create a hostPath PersistentVolume named task-pv-volume with storage 10Gi, access modes ReadWriteOnce, storageClassName manual, and volume at /mnt/data and verify</p> </li> <li> <p>Create a Pod nginx and specify a CPU request and a CPU limit of 0.5 and 1 respectively</p> </li> <li> <p>Create a nginx pod with label env=test in engineering namespace</p> </li> <li> <p>Create a configmap called myconfigmap with literal value appname=myapp</p> </li> <li> <p>Create a pod with environment variables as var1=value1.Check the environment variable in pod</p> </li> <li> <p>List all the pods sorted by created timestamp</p> </li> <li> <p>Create nginx image pods in which one of them is labelled with env=prod</p> </li> <li> <p>Add a taint to node \u201cworker-2\u201d with effect as \u201cNoSchedule\u201d and list the node with taint effect as \u201cNoSchedule\u201d</p> </li> </ol> <pre><code>TODO\n</code></pre> <ol> <li> <p>Delete the above pod and create again from the same yaml file and verifies there is no \u201ctest-file.txt\u201d in the path /data/redis (Since non-persistent storage \u201cemptyDir\u201d is used).</p> </li> <li> <p>Change the label for one of the pod to env=uat and list all the pods to verify</p> </li> <li> <p>List \u201cnginx-dev\u201d and \u201cnginx-prod\u201d pod and delete those pods</p> </li> <li> <p>List all the pods sorted by name</p> </li> <li> <p>Create an nginx pod and set an env value as \u2018var1=val1\u2019. Check the env value existence within the pod</p> </li> <li> <p>Get list of all the nodes with labels</p> </li> <li> <p>Get list of persistent volumes and persistent volume claim in the cluster</p> </li> <li> <p>Create a ETCD backup of kubernetes cluster Note :You don\u2019t need to memorize command, refer \u2013 https://kubernetes.io/docs/tasks/administer-cluster/configureupgrade-etcd/ during exam</p> </li> <li> <p>Update the deployment with the image version 1.17.4 and verify</p> </li> <li> <p>Create a redis pod, and have it use a non-persistent storage Note: In exam, you will have access to kubernetes.io site, Refer : https://kubernetes.io/docs/tasks/configure-pod-container/configurevolume-storage/</p> </li> <li> <p>Change the Image version to 1.15-alpine for the pod you just created and verify the image version is updated.</p> </li> <li>Check the Image version of nginx-dev pod using jsonpath</li> <li>Print all pod name and all image name and write it to a file name \u201c/opt/pod-details.txt\u201d</li> <li>Create a redis pod, and have it use a non-persistent storage (volume that lasts for the lifetime of the Pod)</li> <li>Scale the deployment to 5 replicas</li> <li>Create a Job with an image node which prints node version and verifies there is a pod created for this job</li> <li>Create a NetworkPolicy which denies all ingress traffic</li> <li>Get list of PVs and order by size and write to file \u2013 /opt/pvlist.txt</li> <li>Check the history of the specific revision of that deployment</li> <li>Check the image version in pod without the describe command</li> <li>List all the events sorted by timestamp and put them into file.log and verify</li> <li>Set CPU and memory requests and limits for existing pod name \u201cnginx-prod\u201d. Set requests for CPU and Memory as 100m and 256Mi respectively Set limits for CPU and Memory as 200m and 512Mi respectively</li> <li>Delete persistent volume and persistent volume claim</li> <li> <p>Create a nginx pod that will be deployed to node with the label \u201cgpu=true\u201d</p> </li> <li> <p>Clean the cluster by deleting deployment and hpa you just created</p> </li> <li>Apply the autoscaling to this deployment with minimum 10 and maximum 20 replicas and target CPU of 85% and verify hpa is created and replicas are increased to 10 from 1</li> </ol>"},{"location":"quiz/quiz3/","title":"Quiz3","text":"<ol> <li>Create a busybox pod that runs the command \"env\" and save the output to \"envpod\" file.</li> <li>Watch the job that runs 10 times one by one and verify 10 pods are created and delete those after it\u2019s completed.</li> <li>List all the pods sorted by name.</li> <li>Undo/Rollback deployment to specific revision \"1\".</li> <li>Create an nginx pod and load environment values from the above configmap \"keyvalcfgmap\" and exec into the pod and verify the environment variables and delete the pod.</li> <li>Create a Cronjob with busybox image that prints date and hello from Kubernetes cluster message for every minute.</li> <li>Create an nginx pod and list the pod with different levels of verbosity.</li> <li>Check the history of deployment.</li> <li>List all service account and create a service account called \"admin\".</li> <li>Get the pods with labels env=dev and env=prod and output the labels as well.</li> <li>List all configmap and secrets in the cluster in all namespace and write it to a file /opt/configmapsecret.</li> <li>Create an nginx pod with container Port 80 and it should only receive traffic only if it checks the endpoint / on port 80 and verify and delete the pod.</li> <li>Create a job named \"hello-job\" with the image busybox which echos \"Hello I\u2019m running job\".</li> <li>Get the pods with label env=dev and output the labels.</li> <li>Expose deployment as service named \"myservice\".</li> <li>Label a node as app=test and verify.</li> <li>Scale the deployment from 5 replicas to 20 replicas and verify.</li> <li>List the nginx pod with custom columns POD_NAME and POD_STATUS.</li> <li>Create an nginx pod with containerPort 80 and with a PersistentVolumeClaim \"task-pv-claim\" and has a mount path \"/usr/share/nginx/html\".</li> <li>Change the Image version back to 1.17.1 for the pod you just updated and observe the changes.</li> <li>Get the list of pods of webapp deployment.</li> <li>Get all the pods with label \"env\".</li> <li>Get the memory and CPU usage of all the pods and find out top 3 pods which have the highest usage and put them into the cpuusage.txt file.</li> <li>Check the rollout history and make sure everything is ok after the update.</li> <li>Create a secret mysecret with values user=myuser and password=mypassword.</li> <li>Create a Pod with three busybox containers with commands \"ls; sleep 3600;\", \"echo Hello World; sleep 3600;\", and \"echo this is the third container; sleep 3600\" respectively and check the status.</li> <li>Pause the rollout of the deployment.</li> <li>Get the number of schedulable nodes and write to a file /opt/schedulable-nodes.txt.</li> <li>Create a deployment named \"myapp\" that having 2 replicas with nginx.</li> <li>Get list of PVs and order by size and write to file \"/opt/pvstorage.txt\".</li> <li>Make the node schedulable by uncordon the node.</li> <li>Get IP address of the pod \u2013 \"nginx-dev\".</li> <li>Create a pod with image nginx called nginx and allow traffic on port 80.</li> <li>Check logs of each container that \"busyboxpod-{1,2,3}\".</li> <li>Create a redis pod and expose it on port 6379.</li> <li>Get the DNS records for the service and pods for the deployment redis and put the value in /tmp/dnsrecordpod and /tmp/dnsrecord-service.</li> <li>View certificate details in /etc/kubernetes/pki.</li> <li>Delete the pod without any delay (force delete).</li> <li>Annotate the pod with name=webapp.</li> <li>Create a pod with init container which waits for a service called \"myservice\" to be created. Once the init container completes, the myapp-container should start and print a message \"The app is running\" and sleep for 3600 seconds.</li> <li>Deploy a pod with image=redis on a node with label disktype=ssd.</li> <li>Create a file called \"config.txt\" with two values key1=value1 and key2=value2. Then create a configmap named \"keyvalcfgmap\" and read data from the file \"config.txt\" and verify that configmap is created correctly.</li> <li>Deployment:<ul> <li>Create a deployment of webapp with image nginx:1.17.1 with container port 80 and verify the image version.</li> </ul> </li> <li>Create a namespace called \u2018development\u2019 and a pod with image nginx called nginx on this namespace.</li> <li>Create a Pod with main container busybox which executes this \"while true; do echo \u2018Hi I am from Main container\u2019 &gt;&gt; /var/log/index.html; sleep 5; done\" and with a sidecar container with nginx image which exposes on port 80. Use an emptyDir Volume and mount this volume on path /var/log for busybox and on path /usr/share/nginx/html for nginx container. Verify both containers are running.</li> <li>Create the nginx pod with version 1.17.4 and expose it on port 80.</li> <li>Modify \"hello-job\" and make it run 10 times one after another and 5 times with parallelism: 5.</li> <li>Get a list of all the pods showing name and namespace with a jsonpath expression.</li> <li>Create a pod in a specific node (node1) by placing the pod definition file in a particular folder \"/etc/kubernetes/manifests\".</li> <li>Create a pod with an init container which creates a file \"test.txt\" in \"workdir\" directory. Main container should check if the file \"test.txt\" exists and execute sleep 9999 if the file exists.</li> <li>List pod logs named \"frontend\" and search for the pattern \"started\" and write it to a file \"/opt/errorlogs\".</li> <li>Remove taint added to node \"worker-2\".</li> <li>Create a pod that echoes \"hello world\" and then exits. Have the pod deleted automatically when it\u2019s completed.</li> <li>Create a pod that having 3 containers in it? (Multi-Container).</li> <li>Create a PersistentVolumeClaim of at least 3Gi storage and access mode ReadWriteOnce and verify status is Bound.</li> <li>Create an nginx pod which reads username as the environment variable.</li> <li>Create a busybox pod which executes this command sleep 3600 with the service account admin and verify.</li> <li>Undo the deployment with the previous version and verify everything is Ok.</li> </ol>"},{"location":"quiz/quiz4/","title":"Quiz4","text":"<ol> <li>Create a deployment named Nagari that runs the busybox image and expose port 8080, has 5 replicas and executes the command echo \u201cHello World\u201d. Use imperative commands.</li> <li>Create a nginx pod with label env=test in engineering namespace.</li> <li>Create a pod that echo \u201chello world\u201d and then exists. Have the pod deleted automatically when it\u2018s completed.</li> <li>List nginx-dev and nginx-prod pods and delete those pods.</li> <li>Create a pod with image nginx called nginx and allow traffic on port 80.</li> <li>Create a namespace called \u2018development\u2018 and a pod with image nginx called nginx on this namespace.</li> <li>Get list of all the pods showing name and namespace with a jsonpath expression.</li> <li>Check the image version in pod without the describe command.</li> <li>From the pod label name=cpu-utilizer, find the pod consuming most CPU.</li> <li>Create and configure the service front-end-service so it\u2018s accessible through NodePort and routes to the existing pod named front-end. Assume front-end pod is exposing port 80.</li> <li>List pod logs named frontend and search for the pattern \"started\" and write it to a file /opt/error-logs.</li> <li>List the nginx pod with custom columns POD_NAME and POD_STATUS.</li> <li>List all persistent volumes sorted by capacity, saving the full kubectl output to /opt/exam/volume_list.</li> <li>Create a job that calculates pi to 2000 decimal points using the container with the image named perl and the following commands issued to the container: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]. Once the job has completed, check the logs and export the result to pi-result.txt.</li> <li>Create a new service account with the name podviewer.</li> <li>Create a ClusterRole named podviewer-role that can list persistent volumes.</li> <li>Create a role binding named podviewer-role-binding that grants the cluster role you created in the previous question with the service account podviewer.</li> <li>Create a role named pod-reader which grants read access to pods in the colmenajero namespace.</li> <li>Use imperative commands to create a role named pod-reader which grants read access to pods in the colmenajero namespace.</li> <li>Bind the previously created pod-viewer role to a user named John in the colmenarejo namespace.</li> <li>Deploy a pod named nginx-pod using the nginx:alpine image.</li> <li>Deploy a pod named spectrum using the nginx:alpine image with the labels set to gametype=arcade. Use imperative commands.</li> <li>Get the list of pods with label gametype=arcade in the default namespace.</li> <li>Create a namespace named videogames. Use the shortcut available for namespaces to create the namespace.</li> <li>Get a list of all nodes in the cluster in JSON format and store it in /opt/CKA/nodes.json.</li> <li>Use selectors to get all pods with a videogames value in the metadata.namespace attribute. Consider that the pods could be deployed in non-default namespaces.</li> <li>Expose the spectrum pod you created in previous questions by creating a service named spectrum-service to expose the spectrum application within the cluster on port 6379. Select the two valid answers.</li> <li>Create a deployment named tetris-web-app using the image ckaexa/tetris-webapp with 4 replicas.</li> <li>Create a yaml file called runtime-secret.yaml for a secret called runtime-user-pass. The secret has two fields: username and password. The username should be admin and the password should be supersecret. You must provide the values of the secret directly in the imperative command and generate runtime-secret.yaml from there.</li> <li>Create a secret that will store container environment variables and will be later used for creating a pod with that secret. The values in the secret must be admin for the username key and supersecret for the password key. Use a file and kubectl apply instead of using imperative commands.</li> <li>Create a pod named secret-pod with a container named busybox using busybox image, and use the previously created secret (named mysecret) to add it to the container env variables. The container must execute a command to print the environmental variables so you can check the secret has been correctly configured.</li> <li>Create a static pod named static-nginx on the controlplane node that uses nginx image and the command echo \u201chello control plane\u201d.</li> <li>There is a new application named orange that has been recently deployed. There is an issue with the application and the pod status is Init:CrashLoopBackOff. A CKA admin has provided you the following information coming from a kubectl describe po orange command. Identify and fix the issue.</li> <li>List all the pods sorted by created timestamp.</li> <li>You have been requested to upgrade the control plane node named k8s-control. Which commands you should use to drain the control plane so you can perform the upgrade?</li> <li>Which command should you use to generate the command required to join a node to a cluster by using a token previously generated with kubeadm token generate?</li> <li>Create a multicontainer pod named multi-pod. The first container is named redis and uses the redis image. Also, it will listen on port 6369. The second container is named frontend and uses a django image listening on pod 8000. Which spec template is the right one for that container?</li> <li>Create a pod named security-context-demo-4 using gcr.io/google-samples/node-hello:1.0 that is allowed to modify system time.</li> <li>Label a node named k8s-worker2 with the value core-services=false.</li> <li>Create a pod named runAsPod that has a container named busybox1000 and runs all processes as user 1000 and as group 3000. The pod will execute the following command: sleep 4800.</li> <li>You have created a mongo-db deployment and a mongo-db service that is exposing the database on port 27017 using a ClusterIP type.</li> <li>You want to forward the local port 28015 to the service port. Which command could you execute? (select two)</li> <li>Create a pod named dummy-pod that outputs some data to the host using a volume. The pod must use image busybox and should mount the volume in /output/.</li> <li>You have created a multicontainer pod with two containers named multi-pod. One of the containers is named test and the other one prod. You want to list the files in the root folder of the test container, which command should you use?</li> <li>You have created a pod named milena with the wrong image name. You\u2018ve realized about your mistake and want to remove the pod as soon as possible. Which command you should use?</li> <li>You have created a deployment named condor. The deployment currently has 3 replicas but you have identified the need to scale it out to 5. Which imperative command could you use?</li> <li>Create a pod named frontend with a container named app that requests 64 megabytes of memory but can only use up to 128 megabytes.</li> <li>Create a pod that creates a file in /tmp, then will wait 20 seconds, remove the file, and wait another 300 seconds. You need to configure a livenessProbe that will monitor the file in /tmp/alive and throw a warning if it\u2018s removed. The probe will be executed every 7 seconds.</li> <li>You have created a deployment named nginx using nginx:1.8 as the image. You have been asked to do an update to nginx:1.9.1. Which command should you execute?</li> <li>List the InternalIP of all nodes of the cluster. Save the result to a file /root/exam/node_ips.</li> <li>We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it. Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80.</li> <li>List the DNS IP address(s) used by all pods in ALL namespaces. Save the output to /tmp/nameservers.txt.</li> <li>You need to create a multi-container with two containers: Container 1, name: ying, image: redis. Container 2, name: yang, image: busybox. You also need to configure the following environment variables: Container 1: name: future. Container 2: name: past. Container 2 should execute sleep 3000 command. Select the right solution manifest to create such multi-container pod.</li> <li>A new pod called goku-pod has been deployed in the default namespace and exposed through a service called freezer-svc. For some reason incoming connections to this service are not working. In order to fix it you need to create NetworkPolicy, by the name ingress-to-freezersvc that allows incoming connections to the service over port 80. Select the right manifest to achieve that.</li> <li>List out all the iptables rules defined for ALL services running in the cluster. Save the output to /tmp/fw-rules.txt.</li> <li>Which command would you use to show all the DaemonSets in the cluster?</li> <li>Which command would you use to list ALL nodes in the cluster?</li> <li>Which command would you use to show the pod names and labels?</li> <li>Deploy an Ingress resource called video-svc-ingress with the following configuration: backend path: /streaming, backend service: streaming-service, backend port: 8080, hostname: streaming.example.com, backend path: /live, backend service: livevideo-service, backend port: 9000, hostname: livevideo.example.com.</li> <li>Create a NetworkPolicy called allow-port which allows access ONLY to port 8080. The pods must meet the following criteria: 1. They CANNOT communicate on any port other than 8080. 2. Pods running in ALL other namespaces can also access port 8080.</li> <li>Create a pod called facebook using nginx image. It should be accessible on local port 80 as well as on node\u2018s port. Make sure the port should be same for all nodes in the cluster.</li> <li>You have been asked to create a pod named db-storage using the redis:alpine image. Create the pod manifest using imperative commands and redirect the output to a db-storage.yaml file. After that, mount a volume named tmp-volume with type emptyDir in /data/db.</li> <li>A pod definition file has been created in /tmp/pod-use-pv.yaml. Also, there is a persistent volume already created in the cluster named pv-1. You have been asked to create a persistentVolume claim named my-pvc so it can be later used to binding the pv-1 to the pod.</li> <li>Taking into account the previous question data, modify the pod manifest to mount pv-1 using the mountPath: /data and the persistentVolumeClaim we created in the previous question (my-pvc).</li> <li>Create a new deployment called nginx-dp, with image nginx:1.19 and 2 replicas. Record the version. Next, upgrade the deployment to version 1.21 using rolling update. Make sure that the version upgrade is recorded in the resource annotation.</li> <li>Taking into account the previous question scenario, perform a rollback of the update you made.</li> </ol>"},{"location":"quiz/quiz5/","title":"Quiz5","text":"<ol> <li> <p>Print the names of all deployments in the starwars namespace in the following format: DEPLOYMENT CONTAINER_IMAGE PULL_POLICY READY_REPLICAS NAMESPACE. The data should be sorted by the increasing order of the deployment name.</p> <ul> <li>Example of the result: DEPLOYMENT CONTAINER_IMAGE PULL_POLICY READY_REPLICAS NAMESPACE deploy0 nginx:alpine Always 1 admin2406</li> </ul> </li> <li> <p>A kubeconfig file called admin.kubeconfig has been created in /tmp/CKA-exam. Investigate what's wrong with the configuration based on netstat command results to ensure all required services are correctly running in the cluster.</p> </li> <li> <p>You have access to multiple clusters from your main terminal through kubectl contexts. Write all those context names into /opt/course/1/contexts. Next, write a command to display the current context into /opt/course/1/context_default_kubectl.sh using kubectl. Finally, write a second command doing the same thing into /opt/course/1/context_default_no_kubectl.sh, but without the use of kubectl.</p> </li> <li> <p>Use context: kubectl config use-context k8s-c1-H. Create a single Pod of image httpd:2.4.41-alpine in Namespace default named pod1 with the container named pod1-container. This Pod should only be scheduled on a master node. Explain why Pods are by default not scheduled on master nodes in /opt/course/2/master_schedule_reason.</p> </li> <li> <p>There are two Pods named o3db-* in Namespace project-c13. Scale the Pods down to one replica to save resources and record the action.</p> </li> <li> <p>There are various Pods in all namespaces. Write a command into /opt/course/5/find_pods.sh which lists all Pods sorted by their AGE (metadata.creationTimestamp). Write a second command into /opt/course/5/find_pods_uid.sh which lists all Pods sorted by field metadata.uid using kubectl sorting for both commands.</p> </li> <li> <p>In Namespace default, create a single Pod named ready-if-service-ready of image nginx:1.16.1-alpine. Configure a LivenessProbe which simply runs true.</p> </li> <li> <p>Using the same pod definition from the previous question, configure a ReadinessProbe to check if the URL http://service-am-i-ready:80 is reachable using wget -T2 -O- http://service-am-i-ready:80. Confirm the Pod isn't ready because of the ReadinessProbe and specify the required modifications to the pod manifest.</p> </li> <li> <p>The metrics-server hasn't been installed yet in the cluster. Write the kubectl commands to show node resource usage and Pod and their containers resource usage into /opt/course/7/node.sh and /opt/course/7/pod.sh, respectively.</p> </li> <li> <p>With the following nodes running in a Kubernetes cluster, temporarily stop the kube-scheduler on the master node. Create a single Pod named manual-schedule of image httpd:2.4-alpine and manually schedule that Pod on node cluster2-master1 ensuring it's running.</p> </li> <li> <p>Discuss the types of selectors that can be used to select objects with a given label.</p> </li> <li> <p>Clarify whether a Service can have its own IP address.</p> </li> <li> <p>Explain what a service uses to logically group a set of Pods.</p> </li> <li> <p>Identify the default Service Type in Kubernetes.</p> </li> <li> <p>Confirm whether we can manage the application from the CLI which we deployed using the GUI (Dashboard).</p> </li> <li> <p>Determine which subcommand of 'kubectl' is needed to look at an object's details.</p> </li> <li> <p>Debate whether in Kubernetes, we must create the Deployment first, and the respective Service later.</p> </li> <li> <p>Discuss whether inside a Pod, a volume is shared among containers.</p> </li> <li> <p>Identify a valid Volume type in Kubernetes.</p> </li> <li> <p>Explain how Persistent Volumes can be provisioned.</p> </li> <li> <p>Identify which sub-command of 'kubectl' can be used to scale an application.</p> </li> <li> <p>Discuss whether defining the Pod, we can give a logical name to a port in a Deployment.</p> </li> <li> <p>Confirm whether we can change the port number while forwarding requests from a Service to connected Pods.</p> </li> <li> <p>Discuss the OSI model layer on which the ingress controller creates a Load Balancer.</p> </li> <li> <p>Explore what can be used to access an application running inside Kubernetes from the external world.</p> </li> <li> <p>List the features provided by Ingress.</p> </li> <li> <p>Identify what objects can be restricted with the Object Count Quota.</p> </li> <li> <p>Discuss whether we cannot roll back a Deployment.</p> </li> <li> <p>Define what Helm is.</p> </li> <li> <p>Highlight a crucial feature of Kubernetes.</p> </li> <li> <p>Identify which components of the Kubernetes control plane serve the Kubernetes API using JSON over HTTP.</p> </li> <li> <p>Discuss the Kubernetes setting that defines the image is pulled every time the pod is started.</p> </li> <li> <p>Identify which component of the Kubernetes control plane selects which node an unscheduled pod runs on, based on resource availability.</p> </li> <li> <p>Discuss on which Kubernetes objects Labels can be attached.</p> </li> <li> <p>Identify which component in Kubernetes is an implementation of a network proxy and a load balancer.</p> </li> <li> <p>Discuss which Kubernetes service helps in restricting the service within the cluster.</p> </li> <li> <p>Identify which component of the Kubernetes control plane is a persistent, lightweight, distributed, key-value data store developed by CoreOS that reliably stores the configuration data of the cluster.</p> </li> <li> <p>Discuss which Kubernetes service exposes the service on a static port on the deployed node.</p> </li> <li> <p>Clarify what should be the value of the -register-node flag for Kubernetes master to register the node automatically.</p> </li> <li> <p>Identify which storage system is not supported by Kubernetes.</p> </li> <li> <p>Discuss which Kubernetes pod can be simply created with the kubectl run command.</p> </li> <li> <p>Identify which components of the Kubernetes control plane is a reconciliation loop that drives the actual cluster state towards the desired cluster state.</p> </li> <li> <p>Discuss which component in Kubernetes is responsible for the running state of each node.</p> </li> <li> <p>Identify which of the following Kubernetes daemon embeds the core control loops.</p> </li> <li> <p>Discuss the default port allocated to the API server.</p> </li> <li> <p>Identify the preferred Kubernetes object to manage stateful applications.</p> </li> <li> <p>Confirm whether Kubernetes implements its own networking model.</p> </li> <li> <p>Define the framework that takes advantage of CustomResourceDefinitions to manage applications running inside a Kubernetes cluster.</p> </li> <li> <p>Identify the base Kubernetes object for running workloads.</p> </li> <li> <p>Discuss the name of the object used, in conjunction with a controller, to manage incoming traffic to workloads.</p> </li> <li> <p>Confirm whether Kubernetes is secured by default.</p> </li> <li> <p>List the benefits of doing container orchestration.</p> </li> <li> <p>Confirm whether Kubernetes can be configured in a multi-master configuration.</p> </li> <li> <p>Discuss the implications for a single master node cluster if just the master node dies.</p> </li> <li> <p>Clarify whether kubelet and kube-proxy also run on the master nodes.</p> </li> <li> <p>Identify which component of the Control Plane can communicate directly with the Key/Value store, which stores cluster state.</p> </li> <li> <p>Discuss whether we can run containers of the same Pod on different nodes.</p> </li> <li> <p>Identify which Container Network Specification Kubernetes uses.</p> </li> <li> <p>Define what is referred to as the Core API Group in the API Server.</p> </li> <li> <p>Discuss which process access management is responsible for implementing policies.</p> </li> <li> <p>Lab Environment: Address the issue with worker node k8s-node-01 being in Not Ready state and perform necessary steps to make k8s-node-01 available again.</p> </li> <li> <p>Set the node named k8s-node-1 as unavailable and reschedule all the pods running on it.</p> </li> <li> <p>Lab Environment: Taint the node k8s-node-01 with key equals to env, value equals to qa, and set a hard effect to avoid</p> </li> </ol>"},{"location":"storage/emptydir/","title":"<code>emptyDir</code> Volumes","text":"<p>Kubernetes <code>emptyDir</code> volumes are a temporary storage solution that is created when a Pod is assigned to a node and is destroyed when the Pod is removed from that node. This type of volume is useful for sharing files between containers running in the same Pod.</p>"},{"location":"storage/emptydir/#key-features","title":"Key Features","text":"<ul> <li>Temporary Storage: The lifecycle of an <code>emptyDir</code> volume is tied to the Pod. If the Pod is deleted, the data stored in the <code>emptyDir</code> volume is also deleted.</li> <li>Sharing Data: Containers in the same Pod can use an <code>emptyDir</code> volume to share files.</li> <li>Multiple Mediums: By default, <code>emptyDir</code> volumes are stored on the disk of the host machine. However, you can specify <code>memory</code> as the medium to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you.</li> </ul>"},{"location":"storage/emptydir/#use-cases","title":"Use Cases","text":"<ul> <li>Scratch Space: For temporary storage of data that should not persist beyond the life of the Pod.</li> <li>Checkpointing: For saving data processed by one container before passing it to another container in the same Pod.</li> <li>Log Aggregation: For collecting logs from various components of an application running in separate containers of the same Pod.</li> </ul>"},{"location":"storage/emptydir/#configuration-example","title":"Configuration Example","text":"<p>Here is a basic example of how to define an <code>emptyDir</code> volume in a Pod manifest:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-volume\n  volumes:\n  - name: shared-volume\n    emptyDir: {}\n</code></pre>"},{"location":"storage/emptydir/#best-practices","title":"Best Practices","text":"<ul> <li>Use for Temporary Data Only: Since data in <code>emptyDir</code> volumes is deleted when Pods are removed, only use them for data that does not need to persist.</li> <li>Consider Memory Usage: If using <code>emptyDir</code> with the <code>memory</code> medium, be mindful of the Pod's memory usage since this can affect the node's overall resources.</li> </ul> <p>For more details visit https://kubernetes.io/docs/concepts/storage/volumes/#emptydir</p>"},{"location":"storage/persistent-volumes/","title":"Persistent Volumes","text":"<p>Kubernetes supports managing storage through Persistent Volumes (PV) and Persistent Volume Claims (PVC), offering a high level of abstraction to handle storage resources.</p>"},{"location":"storage/persistent-volumes/#persistent-volumes-pv-and-persistent-volume-claims-pvc-comparison","title":"Persistent Volumes (PV) and Persistent Volume Claims (PVC) Comparison","text":"<p>In Kubernetes, the concepts of PersistentVolume (PV) and PersistentVolumeClaim (PVC) work together to provide persistent storage, but they serve different purposes and operate from distinct perspectives. Below are the key differences between the two:</p>"},{"location":"storage/persistent-volumes/#persistentvolume-pv","title":"PersistentVolume (PV)","text":"<ul> <li>Storage Resources: A PV is a piece of storage in the cluster that has been provisioned by an administrator or dynamically through StorageClasses. It represents a physical resource in the underlying infrastructure, such as a disk in the cloud, NFS storage, among others.</li> <li>Independent Lifecycle: The lifecycle of a PV is independent of any individual pod that consumes it. This means the storage resource can survive after the pods are destroyed.</li> <li>Managed by Administrators: PVs are generally provisioned and managed by cluster administrators. This includes the creation, configuration, and management of the storage lifecycle policy.</li> <li>Reclaim Policies: PVs include policies that define what happens to the volume once the PVC is released. These policies can be Retain, Recycle, or Delete.</li> </ul>"},{"location":"storage/persistent-volumes/#persistentvolumeclaim-pvc","title":"PersistentVolumeClaim (PVC)","text":"<ul> <li>Storage Request: A PVC is essentially a storage request by the user. Application developers use PVCs to request specific sizes and access modes for the storage they need for their applications.</li> <li>User Abstraction: PVCs provide an abstraction layer over the specific details of the physical storage. Users do not need to know where or how the PV is stored; they just need to request the size and access they require.</li> <li>Lifecycle Tied to User: The lifecycle of a PVC is more closely tied to the lifecycle of the pods that use it. A PVC will exist as long as it is needed for the pods and will be deleted by the user if it is no longer necessary.</li> <li>Dynamic Binding: A PVC can be satisfied by any PV that meets the requirements of the PVC (size, access modes, StorageClass), and this \"binding\" process is managed by Kubernetes dynamically.</li> </ul>"},{"location":"storage/persistent-volumes/#direct-comparison","title":"Direct Comparison","text":"<ul> <li>Purpose: PV = offer of storage; PVC = request for storage.</li> <li>Management: PVs are managed by administrators; PVCs are requested by end-users.</li> <li>Lifecycle: PVs have a lifecycle independent of the pods; PVCs are designed to be used and possibly discarded by the pods.</li> <li>Policies: PVs can define reclaim policies; PVCs define the required size and access.</li> <li>Allocation: A PV is a resource in the cluster; a PVC is a ticket to utilize those resources.</li> </ul> <p>In summary, PVs and PVCs are two sides of the same coin, designed to work in conjunction within Kubernetes to provide persistent storage in a manner that is secure, efficient, and easy to use for developers and system administrators.</p>"},{"location":"storage/persistent-volumes/#persistent-volumes-pv-and-persistent-volume-claims-pvc-description","title":"Persistent Volumes (PV) and Persistent Volume Claims (PVC) Description","text":""},{"location":"storage/persistent-volumes/#persistent-volume","title":"Persistent Volume","text":"<p>A Persistent Volume (PV) in Kubernetes is a piece of storage that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a cluster resource that can be used by applications running in pods.</p>"},{"location":"storage/persistent-volumes/#capacity","title":"Capacity","text":"<p>Defines the size of the volume. It is specified when the PV is created and is important for matching PVs with Persistent Volume Claims (PVCs).</p>"},{"location":"storage/persistent-volumes/#access-modes","title":"Access Modes","text":"<p>Determines how the volume can be accessed by the containers. The access modes include:</p> <ul> <li><code>ReadWriteOnce (RWO)</code> - The volume can be mounted as read-write by a single node.</li> <li><code>ReadOnlyMany (ROX)</code>- The volume can be mounted as read-only by many nodes.</li> <li><code>ReadWriteMany (RWX)</code> - The volume can be mounted as read-write by many nodes.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-modes","title":"Volume Modes","text":"<p>Specifies whether the volume is presented as a block device or a filesystem. The two modes are:</p> <ul> <li><code>Block</code></li> <li><code>Filesystem</code></li> </ul> <p>A volume with <code>volumeMode: Filesystem</code> is mounted into Pods into a directory. If the volume is backed by a block device and the device is empty, Kubernetes creates a filesystem on the device before mounting it for the first time. By default, Kubernetes uses the <code>Filesystem</code> mode for volumes. When a PersistentVolume (PV) or a PersistentVolumeClaim (PVC) is created without specifying the <code>volumeMode</code></p>"},{"location":"storage/persistent-volumes/#storage-class","title":"Storage Class","text":"<p>The StorageClass used for dynamic provisioning. It specifies the type of storage to be used and configurations like replication factor, disk type, etc.</p>"},{"location":"storage/persistent-volumes/#reclaim-policy","title":"Reclaim Policy","text":"<p>Determines what happens to the volume after it is released from its claim. Options include:</p> <ul> <li><code>Retain</code> - The volume is kept after it is released.</li> <li><code>Delete</code> - The volume and its data are deleted after it is released.</li> <li><code>Recycle</code> - Deprecated.</li> </ul>"},{"location":"storage/persistent-volumes/#mount-options","title":"Mount Options","text":"<p>Custom mount options that the volume should be mounted with.</p>"},{"location":"storage/persistent-volumes/#phase","title":"Phase","text":"<p>Indicates the current phase of the Persistent Volume (e.g., Available, Bound, Released, Failed).</p>"},{"location":"storage/persistent-volumes/#create-a-persistent-volume","title":"Create a Persistent Volume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-example\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: standard\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  hostPath:\n    path: \"/mnt/data\"\n</code></pre>"},{"location":"storage/persistent-volumes/#persistent-volume-claim","title":"Persistent Volume Claim","text":"<p>A PVC is a request for storage by a user. It specifies the size, access modes, and other requirements for storage.</p>"},{"location":"storage/persistent-volumes/#access-modes_1","title":"Access Modes","text":"<p>Same as for PVs, determines how the volume can be accessed by the containers.</p>"},{"location":"storage/persistent-volumes/#volume-modes_1","title":"Volume Modes","text":"<p>Specifies whether the requested volume is a block device or a filesystem.</p>"},{"location":"storage/persistent-volumes/#resources","title":"Resources","text":"<p>Specifies the minimum size of the volume that the claim should match.</p>"},{"location":"storage/persistent-volumes/#selector","title":"Selector","text":"<p>Allows the claim to specify the PV to bind to using labels.</p>"},{"location":"storage/persistent-volumes/#class","title":"Class","text":"<p>The name of the StorageClass used for dynamic provisioning.</p>"},{"location":"storage/persistent-volumes/#create-persistent-volume-claim","title":"Create Persistent Volume Claim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-example\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: standard\n</code></pre>"},{"location":"storage/persistent-volumes/#use-pvc-in-a-pod","title":"Use PVC in a Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-using-pvc\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n    volumeMounts:\n    - mountPath: \"/var/www/html\"\n      name: my-volume\n  volumes:\n  - name: my-volume\n    persistentVolumeClaim:\n      claimName: pvc-example\n</code></pre>"},{"location":"storage/persistent-volumes/#reclaiming","title":"Reclaiming","text":"<p>The <code>persistentVolumeReclaimPolicy</code> field controls the PV's fate once its PVC is deleted:</p> <ul> <li><code>Retain</code>: The volume remains until manually deleted.</li> <li><code>Delete</code>: The volume is deleted automatically with the PVC.</li> <li><code>Recycle</code>: Deprecated in favor of dynamic provisioning.</li> </ul>"},{"location":"storage/persistent-volumes/#dynamic-provisioning","title":"Dynamic Provisioning","text":"<p>Allows automatic creation of storage resources when a PVC is created, based on the StorageClass specifications.</p>"},{"location":"storage/persistent-volumes/#summary","title":"Summary","text":"<p>Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) provide a way for users and administrators to abstract details of how storage is provided from how it is consumed. Through the use of PVs and PVCs, Kubernetes allows for storage provisioning that can be managed independently of the application using the storage.</p> <p>For more detailed information, visit the official Kubernetes documentation on Persistent Volumes: https://kubernetes.io/docs/concepts/storage/persistent-volumes/</p>"},{"location":"workloads-and-scheduling/daemonsets/","title":"DaemonSets","text":"<p>DaemonSets are a resource in Kubernetes that ensure one or several copies of a Pod run on all (or some) nodes in the cluster. They are particularly useful for deploying infrastructure tasks that need to run on every node, such as log collection, monitoring, or any service that needs to be deployed on each node of the cluster.</p>"},{"location":"workloads-and-scheduling/daemonsets/#what-does-a-daemonset-do","title":"What Does a DaemonSet Do?","text":"<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. Similarly, as nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.</p>"},{"location":"workloads-and-scheduling/daemonsets/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Monitoring and Logging: Deploy monitoring and logging tools across every node.</li> <li>Security: Deploy security agents on every node.</li> <li>Storage: Deploy software that is part of a distributed storage system.</li> </ul>"},{"location":"workloads-and-scheduling/daemonsets/#practical-example-of-a-daemonset","title":"Practical Example of a DaemonSet","text":"<p>Let's create a DaemonSet that deploys an Nginx web server Pod on every node of the cluster.</p> <ol> <li>DaemonSet Definition: Create a file named <code>nginx-daemonset.yaml</code> with the following content:</li> </ol> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-daemonset\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      labels:\n        name: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>This YAML file defines a DaemonSet named <code>nginx-daemonset</code>, which will deploy containers based on the latest Nginx image on each node of the cluster.</p> <ol> <li>Deploying the DaemonSet: To deploy this DaemonSet, run the following command in your terminal:</li> </ol> <pre><code>kubectl apply -f nginx-daemonset.yaml\n</code></pre> <ol> <li>Verification: To verify that the DaemonSet Pods have been deployed on all nodes, you can use the following command:</li> </ol> <pre><code>kubectl get pods -o wide -l name=nginx\n</code></pre> <p>This command lists all Pods with the <code>name=nginx</code> label, allowing you to see on which nodes they have been deployed.</p>"},{"location":"workloads-and-scheduling/daemonsets/#cleanup","title":"Cleanup","text":"<p>To remove the DaemonSet and its associated Pods, run:</p> <pre><code>kubectl delete daemonset nginx-daemonset\n</code></pre> <p>This command deletes the DaemonSet and all the Pods it created in the cluster.</p>"},{"location":"workloads-and-scheduling/daemonsets/#official-documentation","title":"Official Documentation","text":"<p>For more information about DaemonSets and their capabilities, refer to the official Kubernetes documentation:</p> <ul> <li>DaemonSets Documentation: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</li> </ul> <p>This example provides you with a foundation on how to use DaemonSets in Kubernetes to ensure Pods run on all or some nodes of your cluster. Experiment by modifying the YAML file to familiarize yourself more with the available options and how they affect your DaemonSet's behavior.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/","title":"Deployment Rollouts","text":"<p>Kubernetes Deployments provide a declarative way to manage your application's lifecycle, such as scaling, updating, and rollback with zero downtime. Understanding how to effectively manage Deployment rollouts is crucial for ensuring your application remains available and up-to-date. This guide covers the essentials and some advanced topics on Kubernetes Deployment rollouts.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#understanding-rollouts-and-revisions","title":"Understanding Rollouts and Revisions","text":"<p>When you create or update a Deployment, Kubernetes gradually rolls out the changes to your application by updating one or more Pods with the new version, while keeping your application available. Each time a new Deployment rollout is triggered, a new <code>Revision</code> is created, allowing you to rollback to a previous state of your Deployment if needed.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#triggering-a-rollout","title":"Triggering a Rollout","text":"<p>A rollout is triggered by any change to the Pod template in the Deployment's <code>spec.template</code> field. Common changes include:</p> <ul> <li>Updating the container image version</li> <li>Changing environment variables</li> <li>Modifying container ports</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#example-updating-a-deployments-container-image","title":"Example: Updating a Deployment's Container Image","text":"<pre><code>kubectl set image deployment/my-deployment my-container=my-image:1.2.3\n</code></pre> <p>This command updates the container named <code>my-container</code> in the <code>my-deployment</code> Deployment to the image <code>my-image:1.2.3</code>, triggering a rollout.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#monitoring-rollouts","title":"Monitoring Rollouts","text":"<p>Monitoring the status of a rollout is essential to ensure that your changes are being applied as expected without disrupting the service.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#checking-rollout-status","title":"Checking Rollout Status","text":"<pre><code>kubectl rollout status deployment/my-deployment\n</code></pre> <p>This command provides real-time feedback about the progress of the rollout, indicating whether it is successfully rolling out, has completed, or has encountered errors.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#managing-rollback","title":"Managing Rollback","text":"<p>In the lifecycle of application deployment and management, it's not uncommon to encounter scenarios where an update or change leads to unexpected behavior or issues. Kubernetes offers a robust mechanism to revert or rollback a Deployment to a previous state, ensuring service stability and minimal downtime. This process is essential for maintaining the reliability and availability of your applications.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#rolling-back-to-the-last-successful-revision","title":"Rolling Back to the Last Successful Revision","text":"<p>If the latest changes to your Deployment cause issues, you can quickly revert to the most recently known good state by undoing the last rollout. This action effectively rolls back the Deployment to the last successful revision before the latest changes were applied.</p> <p>To perform a rollback to the last successful revision, use the following command:</p> <pre><code>kubectl rollout undo deployment/my-deployment\n</code></pre> <p>This command targets the <code>my-deployment</code> Deployment and instructs Kubernetes to revert to the state defined by the most recent successful revision. This operation is immediate and will start the process of scaling down the Pods created by the faulty rollout while scaling up Pods based on the last known good revision.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#when-to-use","title":"When to Use","text":"<ul> <li>Immediate Issue Resolution: Ideal for quickly addressing issues introduced by a recent deployment without having to manually fix the problems.</li> <li>Automatic Recovery: Helps in scenarios where automatic error detection and recovery processes are in place.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#rolling-back-to-a-specific-revision","title":"Rolling Back to a Specific Revision","text":"<p>In some cases, you might want to rollback to a specific revision, not just the last known good state. This is particularly useful if the last deployment introduced issues that were not immediately detected.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#listing-available-revisions","title":"Listing Available Revisions","text":"<p>Before rolling back to a specific revision, you need to know the available revisions for your Deployment. This can be achieved by listing the rollout history:</p> <pre><code>kubectl rollout history deployment/my-deployment\n</code></pre> <p>This command displays a list of revisions for <code>my-deployment</code>, including summary details that help identify each revision.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#performing-the-rollback","title":"Performing the Rollback","text":"<p>Once you've identified the target revision for rollback, you can specify it using the <code>--to-revision</code> flag in the <code>rollout undo</code> command:</p> <pre><code>kubectl rollout undo deployment/my-deployment --to-revision=2\n</code></pre> <p>In this example, <code>my-deployment</code> is rolled back to revision number 2. This command triggers a rollback process, where Kubernetes scales down the Pods from the current state and scales up Pods matching the state of the specified revision.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#considerations","title":"Considerations","text":"<ul> <li>Revision Numbers: Keep in mind that revision numbers are incremented with each rollout. Ensure you're referring to the correct revision by consulting the rollout history.</li> <li>Impact on Traffic: Rollbacks can temporarily affect traffic as Pods are replaced. Kubernetes tries to minimize downtime by managing the scaling process, but brief disruptions can occur, especially if readiness and liveness probes are not properly configured.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#best-practices-for-rollback-management","title":"Best Practices for Rollback Management","text":"<ul> <li>Regularly Review Deployment History: Familiarize yourself with the deployment history of your applications to make informed decisions when a rollback is necessary.</li> <li>Automate Health Checks: Implement automated health checks and monitoring to quickly detect issues post-deployment, facilitating faster rollbacks when required.</li> <li>Test Rollbacks: Regularly test rollback scenarios in a staging environment to ensure that your application can be safely reverted to a previous state.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#pausing-and-resuming-rollouts","title":"Pausing and Resuming Rollouts","text":"<p>Deployments in Kubernetes are dynamic and continuous processes that ideally run with minimal manual intervention. However, there are circumstances where you might need to temporarily halt a Deployment's rollout to address issues, refine the deployment strategy, or batch updates. Kubernetes provides functionality to pause and later resume rollouts, giving you control over the deployment process and minimizing potential disruptions.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#pausing-a-rollout","title":"Pausing a Rollout","text":"<p>Pausing a rollout temporarily halts the ongoing deployment process, keeping the current state of the Deployment stable while preventing any new updates from being rolled out. This is particularly useful for applying cumulative updates or troubleshooting without impacting the overall deployment process.</p> <p>To pause an ongoing rollout of a Deployment, use the following command:</p> <pre><code>kubectl rollout pause deployment/my-deployment\n</code></pre> <p>This command targets <code>my-deployment</code>, effectively freezing its current state. While paused, any updates to the Deployment's pod template (<code>spec.template</code>) do not trigger a new rollout. This allows you to accumulate changes and apply them all at once, rather than incrementally.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#use-cases-for-pausing-a-rollout","title":"Use Cases for Pausing a Rollout","text":"<ul> <li>Incremental Updates: When you want to apply several updates or configurations changes but prefer to review and apply them as a batch to minimize disruptions.</li> <li>Troubleshooting: If you detect issues during a rollout, pausing allows you to investigate and fix these issues without the pressure of an ongoing deployment.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#resuming-a-rollout","title":"Resuming a Rollout","text":"<p>Once the necessary adjustments or fixes have been made to a paused Deployment, you can resume the rollout to apply these changes. Resuming triggers a new rollout based on the current state of the Deployment's pod template, incorporating all changes made during the pause.</p> <p>To resume a paused rollout, issue the following command:</p> <pre><code>kubectl rollout resume deployment/my-deployment\n</code></pre> <p>This command signals Kubernetes to continue with the rollout process for <code>my-deployment</code>, applying any accumulated updates. The resume action initiates the update process, deploying changes in a controlled and gradual manner according to the Deployment's defined strategy (e.g., RollingUpdate).</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#benefits-of-resuming-a-rollout","title":"Benefits of Resuming a Rollout","text":"<ul> <li>Controlled Deployment: Resuming a paused rollout allows for a more controlled and deliberate deployment process, ensuring that changes are rolled out in a manageable fashion.</li> <li>Reduced Risk: By batching updates and applying them after a pause, you reduce the risk of introducing multiple changes simultaneously, which can be harder to troubleshoot if issues arise.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#best-practices","title":"Best Practices","text":"<ul> <li>Monitor Rollout Status: Regularly check the status of your rollouts, especially after resuming a paused deployment, to ensure that the rollout completes successfully.</li> <li>Use Readiness and Liveness Probes: Ensure your Pods have appropriate readiness and liveness probes configured. These probes help manage the rollout process smoothly by only directing traffic to Pods that are ready and healthy.</li> <li>Review Changes Before Resuming: Before resuming a paused rollout, thoroughly review all changes to avoid unintended consequences. This review process is crucial for maintaining the stability and reliability of your application.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#viewing-rollout-history-and-revisions","title":"Viewing Rollout History and Revisions","text":"<p>A critical aspect of managing deployments in Kubernetes is understanding the history of rollouts and the revisions that have been made over time. This knowledge not only aids in tracking changes but also facilitates rollback decisions and ensures the continuity and stability of your application. Kubernetes provides built-in mechanisms to view the rollout history and detailed information about each revision.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#viewing-rollout-history","title":"Viewing Rollout History","text":"<p>To gain insights into the evolution of a Deployment and to track changes made through each update, you can view the rollout history. This history includes all the revisions made to the Deployment, offering a timeline of changes.</p> <p>To list the revisions of your Deployment, use the following command:</p> <pre><code>kubectl rollout history deployment/my-deployment\n</code></pre> <p>Executing this command displays a list of revisions for <code>my-deployment</code>. Each revision is associated with a revision number, and the list provides a high-level summary of what changes were made in each revision (e.g., updates to the pod template).</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#benefits-of-viewing-rollout-history","title":"Benefits of Viewing Rollout History","text":"<ul> <li>Change Tracking: Allows you to see how many times and when the Deployment was updated.</li> <li>Troubleshooting: Helps in identifying potentially problematic revisions that may have introduced issues into the environment.</li> <li>Rollback Planning: Essential for planning a rollback to a specific, stable revision if needed.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#detailed-view-of-a-specific-revision","title":"Detailed View of a Specific Revision","text":"<p>While the summary provided by the rollout history is useful, sometimes you may need more detailed information about what exactly changed in a specific revision. This detailed view can provide insights into the modifications made to the pod template or other Deployment configurations.</p> <p>To view detailed information about a specific revision, use the <code>--revision</code> flag:</p> <pre><code>kubectl rollout history deployment/my-deployment --revision=2\n</code></pre> <p>This command targets <code>my-deployment</code> and fetches detailed information about revision number 2. The output includes specifics about the changes made in that revision, such as the container image updates, environment variable changes, or any other modifications to the pod template.</p>"},{"location":"workloads-and-scheduling/deployments-rollouts/#when-to-use-detailed-revision-information","title":"When to Use Detailed Revision Information","text":"<ul> <li>Analyzing Changes: To understand the specific changes introduced in a particular revision, especially when diagnosing issues or assessing the impact of a rollback.</li> <li>Auditing and Compliance: Detailed revision information can be crucial for audit trails and compliance, where you need to document exactly what changes were made and when.</li> <li>Learning from Past Deployments: Reviewing detailed changes from past revisions can offer valuable lessons and best practices for future deployments.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#best-practices_1","title":"Best Practices","text":"<ul> <li>Regular Reviews: Make it a habit to regularly review the rollout history and detailed revisions of your Deployments. This practice can help preemptively identify potential issues and understand the deployment lifecycle better.</li> <li>Documentation: Document significant changes and their impacts based on the rollout history. This documentation can be invaluable for new team members, auditing purposes, and historical analysis.</li> <li>Automated Monitoring: Consider integrating tools or scripts that can automatically monitor and alert on certain changes in Deployment revisions. This proactive approach can enhance your deployment strategy and incident response.</li> </ul>"},{"location":"workloads-and-scheduling/deployments-rollouts/#conclusion","title":"Conclusion","text":"<p>Effective management of Kubernetes Deployment rollouts includes understanding how to trigger, monitor, pause, and resume rollouts, as well as how to rollback to previous revisions and clean up old ReplicaSets. Mastery of these concepts ensures your deployments are both robust and flexible, capable of delivering seamless updates to your applications.</p>"},{"location":"workloads-and-scheduling/deployments/","title":"Deployments in Kubernetes","text":"<p>Deployments in Kubernetes are a powerful tool for managing container-packed applications, enabling automatic updates, rollbacks, scaling, and much more. Below, we detail how to manage Deployments for various use cases.</p>"},{"location":"workloads-and-scheduling/deployments/#creating-a-deployment","title":"Creating a Deployment","text":"<p>To deploy an application in Kubernetes and ensure its Pods are managed by a ReplicaSet in the background, you can use two approaches: declarative (using a YAML file) or imperative (using a command). Below, both methods are detailed for deploying an Nginx server as an example.</p> <ul> <li>Define the Deployment in a YAML file:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deployment\n  name: nginx-deployment\nspec:\n  replicas: 3  # Specifies the desired number of replicas\n  selector:\n    matchLabels:\n      app: nginx-deployment\n  template:\n    metadata:\n      labels:\n        app: nginx-deployment\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80  # The port the container exposes\n\n</code></pre> <p>This YAML specifies a Deployment that manages a set of Pods. Each Pod runs a container based on the Nginx 1.14.2 image. The <code>replicas: 3</code> line indicates that three replicas of the Pod should be maintained at all times.</p> <ul> <li>Apply the Deployment: After defining your Deployment in a YAML file, use <code>kubectl</code> to create it in your Kubernetes cluster:</li> </ul> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre> <p>This command tells Kubernetes to make the cluster's state match the desired state described in the <code>nginx-deployment.yaml</code> file.</p> <p>Note</p> <p>Alternatively, you can create a Deployment imperatively using a single kubectl command. This method is quicker for simple deployments or for testing purposes:</p> <p><code>kubectl create deployment nginx-deployment --image=nginx:1.14.2 --replicas=3</code></p> <ul> <li>Check the Rollout \ud83d\udcdd status:</li> </ul> <pre><code>kubectl rollout status deployment/nginx-deployment\n</code></pre>"},{"location":"workloads-and-scheduling/deployments/#updating-a-deployment","title":"Updating a Deployment","text":"<p>To update the Pods to a new state, modify the pod specification in the Deployment. This creates a new ReplicaSet and begins moving the Pods to the new state at a controlled rate.</p> <ul> <li>Update the container image in your Deployment file or use kubectl:</li> </ul> <pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n</code></pre> <ul> <li>Each new ReplicaSet updates the revision of the Deployment, which you can verify with:</li> </ul> <pre><code>kubectl rollout history deployment/nginx-deployment\n</code></pre>"},{"location":"workloads-and-scheduling/deployments/#rolling-back-a-deployment","title":"Rolling Back a Deployment \ud83d\udcdd","text":"<p>If the current state is not stable, you can rollback to a previous revision of the Deployment:</p> <ul> <li>Find the revision to rollback to:</li> </ul> <pre><code>kubectl rollout history deployment/nginx-deployment\n</code></pre> <ul> <li>Rollback the Deployment to a previous revision:</li> </ul> <pre><code>kubectl rollout undo deployment/nginx-deployment --to-revision=&lt;revision&gt;\n</code></pre>"},{"location":"workloads-and-scheduling/deployments/#scaling-a-deployment","title":"Scaling a Deployment","text":"<p>To handle more load, you can increase the number of replicas of the Deployment:</p> <pre><code>kubectl scale deployment/nginx-deployment --replicas=5\n</code></pre>"},{"location":"workloads-and-scheduling/deployments/#pausingresuming-a-rollout","title":"Pausing/Resuming a Rollout \ud83d\udcdd","text":"<p>During a Deployment's rollout, you might find the need to temporarily halt the update process. This could be due to a discovered issue that requires immediate attention or to batch several updates together before continuing the rollout. Kubernetes allows you to pause and later resume the rollout process of a Deployment. This feature is particularly useful for managing and controlling the rollout process more granitcally.</p>"},{"location":"workloads-and-scheduling/deployments/#pausing-the-rollout","title":"Pausing the Rollout","text":"<p>Pausing a rollout prevents any further updates to the Pods managed by the Deployment, but it does not affect the Pods that have already been updated. To pause the ongoing rollout of a Deployment, use the following command:</p> <pre><code>kubectl rollout pause deployment/nginx-deployment\n</code></pre> <p>This command will halt the rollout process for the <code>nginx-deployment</code> Deployment, allowing you to perform necessary updates, fixes, or changes to the Deployment's pod template (<code>spec.template</code>) without triggering a new rollout for each change.</p>"},{"location":"workloads-and-scheduling/deployments/#resuming-the-rollout","title":"Resuming the Rollout","text":"<p>After applying the necessary changes and ensuring that everything is set correctly, you can resume the rollout to start updating Pods with the new configuration. To resume a paused rollout, use the following command:</p> <pre><code>kubectl rollout resume deployment/nginx-deployment\n</code></pre>"},{"location":"workloads-and-scheduling/deployments/#using-the-deployments-status-as-an-indicator","title":"Using the Deployment's Status as an Indicator","text":"<p>If a Rollout \ud83d\udcdd has stuck, the Deployment's status can indicate it. Check the status and events of the Deployment to diagnose issues:</p> <pre><code>kubectl describe deployment/nginx-deployment\n</code></pre>"},{"location":"workloads-and-scheduling/deployments/#cleaning-up-old-replicasets","title":"Cleaning Up Old ReplicaSets","text":"<p>Deployments can leave behind old ReplicaSets that are no longer needed. You can manually clean up these resources or adjust the Deployment's policies to do it automatically.</p> <ul> <li>List all ReplicaSets:</li> </ul> <pre><code>kubectl get rs\n</code></pre> <ul> <li>Delete the ReplicaSets you no longer need:</li> </ul> <pre><code>kubectl delete rs &lt;replicaset-name&gt;\n</code></pre> <p>To automatically manage the cleanup of old ReplicaSets, adjust <code>spec.revisionHistoryLimit</code> in your Deployment to the desired number of ReplicaSets history to keep.</p>"},{"location":"workloads-and-scheduling/deployments/#conclusion","title":"Conclusion","text":"<p>Deployments in Kubernetes offer a robust mechanism for deploying, updating, and scaling applications, as well as for safely rolling back to previous states. Effectively using these capabilities can significantly improve the lifecycle management of your applications in Kubernetes.</p> <p>For more information and detailed guides, consult the official Kubernetes documentation: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</p>"}]}